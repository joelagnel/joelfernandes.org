<!DOCTYPE html>
<html>
<head>
<title>Studying cache-line sharing effects on SMP systems - Joel Fernandes</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="author" content="Joel Fernandes">
<meta name="viewport" content="width=device-width">
<link rel="stylesheet" href="/preview/page.css">
<link rel="stylesheet" href="/preview/css/syntax.css">
<link rel="stylesheet" href="/preview/css/main.css">
</head>
<body>

<div class="page">

<div class="nav">
<p class="navhdr">Joel's site:</p>
<a href="/preview/">Home</a><br>
<a href="/preview/bio.html">Bio</a><br>
<a href="/preview/linuxperf.html">Linux Perf</a><br>
<a href="/preview/rcu.html">RCU</a><br>
<a href="/preview/tracing.html">Tracing</a><br>
<a href="/preview/schedulers.html">Schedulers</a><br>
<a href="/preview/memory-ordering.html">Memory Ordering</a><br>
<a href="/preview/gpu.html">GPU Drivers</a><br>
<a href="/preview/resources/">Talks</a><br>
<a href="/preview/joel/">Resume</a><br>
</div>

<div class="recent">
<p class="navhdr">Recent posts:</p>
<ul>

<li class="recent">2023-06-25 &raquo;<br><a href="/preview/blog/2023-06-25/svm-vectors.html">SVM and vectors for the curious</a></li>

<li class="recent">2023-06-10 &raquo;<br><a href="/preview/blog/2023-06-10/selinux-procfs.html">SELinux Debugging on ChromeOS</a></li>

<li class="recent">2023-04-28 &raquo;<br><a href="/preview/blog/2023-04-28/hazard-pointers.html">Understanding Hazard Pointers</a></li>

<li class="recent">2023-04-25 &raquo;<br><a href="/preview/blog/2023-04-25/ppc-stack-guards.html">PowerPC stack guard false positives in Linux kernel</a></li>

<li class="recent">2023-02-24 &raquo;<br><a href="/preview/blog/2023-02-24/ycm-working.html">Getting YouCompleteMe working for kernel development</a></li>

<li class="recent">2023-01-29 &raquo;<br><a href="/preview/blog/2023-01-29/figuring-out-herd7.html">Figuring out herd7 memory models</a></li>

<li class="recent">2022-11-13 &raquo;<br><a href="/preview/blog/2022-11-13/hrtimer.html">On workings of hrtimer's slack time functionality</a></li>

<li class="recent">2020-10-25 &raquo;<br><a href="/preview/blog/2020-10-25/cpp-ref.html">C++ rvalue references</a></li>

<li class="recent">2020-03-06 &raquo;<br><a href="/preview/blog/2020-03-06/srcu-scan.html">SRCU state double scan</a></li>

<li class="recent">2019-10-18 &raquo;<br><a href="/preview/blog/2019-10-18/pluscal-store-ordering.html">Modeling (lack of) store ordering using PlusCal - and a wishlist</a></li>

</ul>
<p><a href="/preview/">All posts</a><br>
<a href="/preview/bio.html">About</a><br>
<a href="/preview/feed.xml">RSS</a></p>
</div>

<div class="site">

<p><a href="/preview/">&laquo; Back to Home</a></p>

<h1 class="title">Studying cache-line sharing effects on SMP systems</h1>
<p class="meta">24 Apr 2014</p>

<div class="post">
<p>Having read the chapter on counting and per-CPU counters in <a href="http://www.lulu.com/shop/paul-e-mckenney/is-parallel-programming-hard-and-if-so-what-can-you-do-about-it-first-bw-print-edition/paperback/product-21562459.html">Paul Mckenney’s recent book</a>, I thought I would do a small experiment to check how good or bad it would be if those per-CPU counters were close to each other in memory.</p>

<p>Paul talks about using one global shared counter for N threads on N CPUs, and the effects it can have on the cache. Each CPU core’s cache in an SMP system will need exclusive rights on a specific cache line of memory, before it can do the write. This means that, at any given time <em>only one</em> CPU can and should do a write to that part of memory.</p>

<p>This is accomplished typically by an invalidate protocol, where each CPU needs to do some inter-processor communication before it can assume it has exclusive access to that cache line, and also read any copies that may still be in some other core’s cache and not in main memory. This is an expensive operation that is to be avoided at all costs!</p>

<p>Then Paul goes about saying, OK- let’s have a per-thread counter, and have each core increment it independently, and when we need a read out, we would grab a lock and add all of the individual counters together. This works great, assuming each per-thread counter is separated by atleast a cache line. That’s guaranteed, when one uses the <code class="language-plaintext highlighter-rouge">__thread</code> primitive nicely separating out the memory to reduce cache line sharing effects.</p>

<p>So I decided to flip this around, and have per-thread counters that were closely spaced and do some counting with them. Instead of using <code class="language-plaintext highlighter-rouge">__thread</code>, I created an array of counters, each element belonging to some thread. The counters are still separate and not shared, but they may still be in shared cache-line causing the nasty effects we talked about, which I wanted to measure.</p>

<p><a href="https://github.com/joelagnel/smp-experiments/blob/05afb2db4fea1c6c0b4614c180186c10627a341a/cache-sharing.c">My program</a> sets up N counting threads, and assumes its running each of them on a single core on typical multicore system.  Various iterations of per-thread counting is done, with the counters separated by increasing powers of 2 each iteration. After each iteration, I stop all threads, add the per-thread counter values and report the result.</p>

<p>Below are the results of running the program on 3 different SMP systems (2 threads on 2 CPUs, sorry I don’t have better multi-core hardware ATM):</p>

<p>Effect of running on a reference ARM dual-core Cortex-A9 system:
<img src="/preview/images/cache-sharing/a9-counts.jpeg" alt="" /></p>

<p>Notice the jump in through-put once the separation changes from 16 to 32 bytes. That gives us a good idea that the L1 cache line size on Cortex-A9 systems is 32 bytes (8 words). Something the author didn’t know for sure in advance (I initially thought it was 64-bytes).</p>

<p>Effect of running on a reference ARM dual-core Cortex-A15 system:
<img src="/preview/images/cache-sharing/a15-counts.jpeg" alt="" /></p>

<p>L1 Cache-line size of Cortex A-15 is 64 bytes (8 words). Expected jump for a separation of 64 bytes.</p>

<p>Effect of running on a x86-64 i7-3687U dual-core CPU:
<img src="/preview/images/cache-sharing/x86-counts.jpeg" alt="" />.</p>

<p>L1 Cache-line size of this CPU is 64 bytes too (8 words). Expected jump for a separation of 64 bytes.</p>

<p>This shows your parallel programs need to take care of cache-line alignment to avoid false-sharing effects. Also, doing something like this in your program is an indirect way to find out what the cache-line size is for your CPU, or a direct way to get fired, whichever way you want to look at it. ;)</p>

</div>

<p><a href="/preview/">&laquo; Back to Home</a></p>

<div class="footer">
<div class="contact">
<p>Joel Fernandes · <a href="https://twitter.com/joel_linux">Twitter</a> · <a href="https://www.linkedin.com/in/joelagnel">LinkedIn</a> · <a href="/preview/about-site.html">About site</a></p>
</div>
</div>

</div>

</div>

</body>
</html>
