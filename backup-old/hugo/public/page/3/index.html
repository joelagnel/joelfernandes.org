<!DOCTYPE html>


<html class="no-js" lang="en">
<head>
	<meta name="generator" content="Hugo 0.123.7"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  <title>JoelFernandes.org - JoelFernandes.org</title>
  <meta name="author" content="Joel Fernandes">

  
  <meta name="description" content="">

  
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="canonical" href="http://localhost:1313/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="\/javascripts\/libs\/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  
  
</head>
<body   >
  <header role="banner"><div>
<div style="margin-right:50px;float:left;">
  <h1><a href="/">JoelFernandes.org</a></h1>
  
</div>
<div style="float:left;" class="hnav">
 <br>
 
 <a href="/categories/">Blog posts by category.</a><br>
 <a href="/blog/archives/">Archive of all blog posts.</a><br>
 <a href="/resources/">Presentations and other work.</a><br>
</div>
<div style="float:right;">
<img src="/images/peng.png" height=100 width=100>
</div>
</div></header>

  <div id="main">
    <div id="content">
      
<div class="blog-index">

<article>
Hello! I'm Joel and this my personal website built with Hugo! I currently
work at Google. My interests are scheduler, RCU, tracing, synchronization,
memory models and other kernel internals. I also love contributing to the
upstream Linux kernel and other open source projects.

Connect with me on [Twitter](https://twitter.com/joel_linux), and
[LinkedIn](https://www.linkedin.com/in/joelagnel). Or, drop me an email at:
joel _at_ joelfernandes _dot_ org

Look for my name in the kernel git log to find my upstream kernel patches.
Check out [my resume](/joel/joel-resume.pdf) for full details of my work
experience. I also actively present at conferences, see a list of my past
[talks, presentations and publications](/resources).

Full list of all blog posts on this site:

<font face="monospace">
 <li>
   <span>25 Jun 2023</span> &nbsp; <a href="http://localhost:1313/blog/2023/06/25/svm-and-vectors-for-the-curious/">SVM and vectors for the curious</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>22 Dec 2018</span> &nbsp; <a href="http://localhost:1313/blog/2018/12/22/dumping-user-and-kernel-stacks-on-kernel-events/">Dumping User and Kernel stacks on Kernel events</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>10 Feb 2018</span> &nbsp; <a href="http://localhost:1313/blog/2018/02/10/usdt-for-reliable-userspace-event-tracing/">USDT for reliable Userspace event tracing</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>31 Dec 2016</span> &nbsp; <a href="http://localhost:1313/blog/2016/12/31/armv8-flamegraph-and-nmi-support/">ARMv8: flamegraph and NMI support</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>18 Jun 2016</span> &nbsp; <a href="http://localhost:1313/blog/2016/06/18/ftrace-events-mechanism/">Ftrace events mechanism</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>20 Mar 2016</span> &nbsp; <a href="http://localhost:1313/blog/2016/03/20/tif_need_resched-why-is-it-needed/">TIF_NEED_RESCHED: why is it needed</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>25 Dec 2015</span> &nbsp; <a href="http://localhost:1313/blog/2015/12/25/tying-2-voltage-sources/signals-together/">Tying 2 voltage sources/signals together</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>04 Jun 2014</span> &nbsp; <a href="http://localhost:1313/blog/2014/06/04/microsd-card-remote-switch/">MicroSD card remote switch</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>07 May 2014</span> &nbsp; <a href="http://localhost:1313/blog/2014/05/07/linux-spinlock-internals/">Linux Spinlock Internals</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>24 Apr 2014</span> &nbsp; <a href="http://localhost:1313/blog/2014/04/24/studying-cache-line-sharing-effects-on-smp-systems/">Studying cache-line sharing effects on SMP systems</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>22 Apr 2014</span> &nbsp; <a href="http://localhost:1313/blog/2014/04/22/design-of-fork-followed-by-exec-in-linux/">Design of fork followed by exec in Linux</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/"></a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/bpfd-running-bcc-tools-remotely-across-systems/">BPFd- Running BCC tools remotely across systems</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/c-rvalue-references/">C&#43;&#43; rvalue references</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/categories/">Categories</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/joel/">false</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/figuring-out-herd7-memory-models/">Figuring out herd7 memory models</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/getting-youcompleteme-working-for-kernel-development/">Getting YouCompleteMe working for kernel development</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/gus-global-unbounded-sequences/">GUS (Global Unbounded Sequences)</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/archives/">List of articles</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/making-sense-of-scheduler-deadlocks-in-rcu/">Making sense of scheduler deadlocks in RCU</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/modeling-lack-of-store-ordering-using-pluscal-and-a-wishlist/">Modeling (lack of) store ordering using PlusCal - and a wishlist</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/on-workings-of-hrtimers-slack-time-functionality/">On workings of hrtimer&#39;s slack time functionality</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/powerpc-stack-guard-false-positives-in-linux-kernel/">PowerPC stack guard false positives in Linux kernel</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/rcu-and-dynticks-idle-mode/">RCU and dynticks-idle mode</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/rcu-preempt-what-happens-on-a-context-switch/">RCU-preempt: What happens on a context switch</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/resources/">Resources</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/selinux-debugging-on-chromeos/">SELinux Debugging on ChromeOS</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/single-stepping-the-kernels-c-code/">Single-stepping the kernel&#39;s C code</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/srcu-state-double-scan/">SRCU state double scan</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/understanding-hazard-pointers/">Understanding Hazard Pointers</a> 
 </li>
</font>

</article>


 


    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">Making sense of scheduler deadlocks in RCU</h1>
    
    
      <p class="meta">
        
        
           | <a href="http://localhost:1313/blog/1/01/01/making-sense-of-scheduler-deadlocks-in-rcu/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/1/01/01/making-sense-of-scheduler-deadlocks-in-rcu/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>Note: At the time of this writing, it is kernel v5.3 release. RCU moves fast
and can change in the future, so some details in this article may be obsolete.</p>
<p>The RCU subsystem and the task scheduler are inter-dependent. They both depend
on each other to function correctly. The scheduler has many data structures
that are protected by RCU. And, RCU may need to wake up threads to perform
things like completing grace periods and callback execution. One such case
where RCU does a wake up and enters the scheduler is
<code>rcu_read_unlock_special()</code>.</p>
<p>Recently Paul McKenney consolidated RCU flavors. What does this mean?</p>
<p>Consider the following code executing in CPU 0:</p>
<pre tabindex="0"><code>preempt_disable();
rcu_read_lock();
rcu_read_unlock();
preempt_enable();
</code></pre><p>And, consider the following code executing in CPU 1:</p>
<pre tabindex="0"><code>a = 1;
synchronize_rcu();  // Assume synchronize_rcu
                    // executes after CPU0&#39;s rcu_read_lock
b = 2;
</code></pre><p>CPU 0&rsquo;s execution path shows 2 flavors of RCU readers, one nested into another.
The <code>preempt_{disable,enable}</code> pair is an <code>RCU-sched</code> flavor RCU reader
section, while the <code>rcu_read_{lock,unlock}</code> pair is an <code>RCU-preempt</code> flavor RCU
reader section.</p>
<p>In older kernels (before v4.20), CPU 1&rsquo;s <code>synchronize_rcu()</code>  could return
<em>after</em> CPU 0&rsquo;s <code>rcu_read_unlock()</code> but before CPU 0&rsquo;s <code>preempt_enable()</code>. This
is because <code>synchronize_rcu()</code> only needs to wait for the &ldquo;RCU-preempt&rdquo; flavor
of the RCU grace period to end.</p>
<p>In newer kernels (v4.20 and above), the RCU-preempt and RCU-sched flavors have
been consolidated. This means CPU 1&rsquo;s <code>synchronize_rcu()</code> is guaranteed to wait
for both of CPU 1&rsquo;s <code>rcu_read_unlock()</code> and <code>preempt_enable()</code> to complete.</p>
<p>Now, lets get a bit more detailed. That <code>rcu_read_unlock()</code> most likely does
very little. However, there are cases where it needs to do more, by calling
<code>rcu_read_unlock_special()</code>. One such case is if the reader section was
preempted. A few more cases are:</p>
<ul>
<li>The RCU reader is blocking an expedited grace period, so it needed to report
a quiescent state quickly.</li>
<li>The RCU reader is blocking a grace period for too long (~100 jiffies on my
system, that&rsquo;s the default but can be set with
<code>rcutree.jiffies_till_sched_qs</code> parameter).</li>
</ul>
<p>In all these cases, the <code>rcu_read_unlock()</code> needs to do more work. However,
care must be taken when calling <code>rcu_read_unlock()</code> from the scheduler, that&rsquo;s
why this article on scheduler deadlocks.</p>
<p>One of the reasons <code>rcu_read_unlock_special()</code> needs to call into the scheduler
is priority de-boosting:  A task getting preempted in the middle of an RCU
read-side critical section results in blocking the completion of the critical
section and hence could prevent current and future grace periods from ending.
So the priority of the RCU reader may need to be boosted so that it gets enough
CPU time to make progress, and have the grace period end soon. But it also
needs to be de-boosted after the reader section completes. This de-boosting
happens by calling of the <code>rcu_read_unlock_special()</code> function in the outer
most <code>rcu_read_unlock()</code>.</p>
<p>What could go wrong with the scheduler using RCU? Let us see this in action.
Consider the following piece of code executed in the scheduler:</p>
<pre tabindex="0"><code>  reader()
	{
		rcu_read_lock();
		do_something();     // Preemption happened
                /* Preempted task got boosted */
		task_rq_lock();     // Disables interrupts
                rcu_read_unlock();  // Need to de-boost
		task_rq_unlock();   // Re-enables interrupts
	}
</code></pre><p>Assume that the <code>rcu_read_unlock()</code> needs to de-boost the task&rsquo;s priority. This
may cause it to enter the scheduler and cause a deadlock due to recursive
locking of RQ/PI locks.</p>
<p>Because of these kind of issues, there has traditionally been a rule that RCU
usage in the scheduler must follow:</p>
<pre tabindex="0"><code>“Thou shall not hold RQ/PI locks across an rcu_read_unlock() if thou not
holding it or disabling IRQ across both both the rcu_read_lock() +
rcu_read_unlock().”
</code></pre><p>More on this rule can be read <a href="https://lwn.net/Articles/453002/">here as well</a>.</p>
<p>Obviously, acquiring RQ/PI locks across the whole <code>rcu_read_lock()</code> and
<code>rcu_read_unlock()</code> pair would resolve the above situation. Since preemption
and interrupts are disabled across the whole <code>rcu_read_lock()</code> and
<code>rcu_read_unlock()</code> pair; there is no question of task preemption.</p>
<p>Anyway, the point is <code>rcu_read_unlock()</code> needs to be careful about scheduler
wake-ups; either by avoiding calls to <code>rcu_read_unlock_special()</code> altogether
(as is the case if interrupts are disabled across the entire RCU reader), or by
detecting situations where a wake up is unsafe. Peter Ziljstra says there&rsquo;s no
way to know when the scheduler uses RCU, so &ldquo;generic&rdquo; detection of the unsafe
condition is a bit tricky.</p>
<p>Now with RCU consolidation, the above situation actually improves. Even if the
scheduler RQ/PI locks are not held across the whole read-side critical sectoin,
but just across that of the <code>rcu_read_unlock()</code>, then that itself may be enough
to prevent a scheduler deadlock. The reasoning is: during the
<code>rcu_read_unlock()</code>, we cannot yet report a QS until the RQ/PI lock is itself
released since the act of holding the lock itself means preemption is disabled
and that would cause a QS deferral. As a result, the act of priority
de-boosting would also be deferred and prevent a possible scheduler deadlock.</p>
<p>However, RCU consolidation introduces even newer scenarios where the
<code>rcu_read_unlock()</code>  has to enter the scheduler, if the &ldquo;scheduler rules&rdquo; above
is not honored, as explained below:</p>
<p>Consider the previous code example. Now also assume that the RCU reader is
blocking an expedited RCU grace period. That is just a fancy term for a grace
period that needs to end fast. These grace periods have to complete much more
quickly than normal grace period. An expedited grace period causes currently
running RCU reader sections to receive IPIs that <a href="https://github.com/joelagnel/linux-kernel/blob/rcu/rcu-check-unsafe-scheduler-use-2/kernel/rcu/tree_exp.h#L641">set a
hint</a>.
Setting of this hint results in the outermost <code>rcu_read_unlock()</code> calling
<code>rcu_read_unlock_special()</code>, which otherwise would not occur.
When <code>rcu_read_unlock_special()</code> gets called in this scenario, it tries to get
more aggressive once it <a href="https://github.com/joelagnel/linux-kernel/blob/rcu/rcu-check-unsafe-scheduler-use-2/kernel/rcu/tree_plugin.h#L627">notices
that</a>
the reader has blocked an expedited RCU grace period. In particular, it
<a href="https://github.com/joelagnel/linux-kernel/blob/rcu/rcu-check-unsafe-scheduler-use-2/kernel/rcu/tree_plugin.h#L620">notices that preemption is
disabled</a>
and so the grace period cannot end due to RCU consolidation. Out of
desperation, it raises a softirq (<code>raise_softirq()</code>) in the hope that the next
time the softirq runs, the grace period could be ended quickly before the
scheduler tick occurs. But that can cause a scheduler deadlock by way of entry
into the scheduler due to a ksoftirqd-wakeup.</p>
<p>The cure for this problem is the same, holding the RQ/PI locks across the
entire reader section results in no question of a scheduler related deadlock
due to recursively acquiring of these locks; because there would be no question
of expedited-grace-period IPIs, hence no question of setting of any hints, and
hence no question of calling <code>rcu_read_unlock_special()</code> from scheduler code.
For a twist of the IPI problem, see <a href="#special-note">special note</a>.</p>
<p>However, the RCU consolidation throws yet another curve ball. Paul McKenney
<a href="https://lore.kernel.org/lkml/20190627173831.GW26519@linux.ibm.com/">explained on
LKML</a> that
there is yet another situation now due to RCU consolidation that can cause
scheduler deadlocks.</p>
<p>Consider the following code, where <code>previous_reader()</code> and <code>current_reader()</code>
execute in quick succession in the context of the same task:</p>
<pre tabindex="0"><code>       previous_reader()
	{
		rcu_read_lock();
		do_something();      // Preemption or IPI happened
		local_irq_disable(); // Cannot be the scheduler
		do_something_else();
		rcu_read_unlock();  // As IRQs are off, defer QS report
                                    //but set deferred_qs bit in 
                                    //rcu_read_unlock_special
		do_some_other_thing();
		local_irq_enable();
	}

        // QS from previous_reader() is still deferred.
	current_reader() 
	{
		local_irq_disable();  // Might be the scheduler.
		do_whatever();
		rcu_read_lock();
		do_whatever_else();
		rcu_read_unlock();    // Must still defer reporting QS
		do_whatever_comes_to_mind();
		local_irq_enable();
	}
</code></pre><p>Here <code>previous_reader()</code> had a preemption; even though the <code>current_reader()</code>
did not - but the <code>current_reader()</code> still needs to call
<code>rcu_read_unlock_special()</code> from the scheduler!  This situation would not
happen in the pre-consolidated-RCU world because <code>previous_reader()</code>&rsquo;s
<code>rcu_read_unlock()</code> would have taken care of it.</p>
<p>As you can see, just following the scheduler rule of disabling interrupts
across the entire reader section does not help. To detect the above scenario; a
new bitfield  <code>deferred_qs</code> has been
<a href="https://lore.kernel.org/patchwork/patch/1057344/">added</a> to the
<code>task_struct::rcu_read_unlock_special</code> union. Now what happens is, at
<code>rcu_read_unlock()</code>-time, the <code>previous reader()</code> sets this bit, and the
<code>current_reader()</code> checks this bit. If set, the call to <code>raise_softirq()</code> is
avoided thus eliminating the possibility of a scheduler deadlock.</p>
<p>Hopefully no other scheduler deadlock issue is lurking!</p>
<p>Coming back to the scheduler rule, I have been running overnight rcutorture
tests to detect if this rule is ever violated. Here is the <a href="https://github.com/joelagnel/linux-kernel/commits/rcu/rcu-check-unsafe-scheduler-use-2">test
patch</a>
checking for the unsafe condition. So far I have not seen this condition occur
which is a good sign.</p>
<h2 id="for-mainline-is-worth-it-thankfully-lpc-2019-is-right-around-the-corner--">I may need to check with Paul McKenney about whether proposing this checking
for mainline is worth it. Thankfully, LPC 2019 is right around the corner! ;-)</h2>
<h4 id="special-note">Special Note</h4>
<p>[1] The expedited IPI interrupting an RCU reader has a variation. For an
example see below where the IPI was not received, but we still have a problem
because the <code>-&gt;need_qs</code> bit in the <code>rcu_read_unlock_special union</code> got set even
though the expedited grace period started after IRQs were disabled. The start
of the expedited grace period would set the <code>rnp-&gt;expmask</code> bit for the CPU. In
the unlock path, because the <code>-&gt;need_qs</code> bit is set, it will call
<code>rcu_read_unlock_special()</code> and risk a deadlock by way of a <code>ksoftirqd</code> wakeup
because <code>exp</code> in that function is true.</p>
<pre tabindex="0"><code>CPU 0                         CPU 1
preempt_disable();
rcu_read_lock();

// do something real long

// Scheduler-tick sets
// -&gt;need_qs as reader is
// held for too long.

local_irq_disable();
                              // Expedited GP started
// Exp IPI not received
// because IRQs are off.

local_irq_enable();

// Here rcu_read_unlock will
// still call ..._special()
// as -&gt;need_qs got set.
rcu_read_unlock();

preempt_enable();
</code></pre><p>The fix for this issue is the same as described earlier, disabling interrupts
across both <code>rcu_read_lock()</code> and <code>rcu_read_unlock()</code> in the scheduler path.</p>
</div>

    </article>

    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">Modeling (lack of) store ordering using PlusCal - and a wishlist</h1>
    
    
      <p class="meta">
        
        
           | <a href="http://localhost:1313/blog/1/01/01/modeling-lack-of-store-ordering-using-pluscal-and-a-wishlist/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/1/01/01/modeling-lack-of-store-ordering-using-pluscal-and-a-wishlist/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>The Message Passing pattern (MP pattern) is shown in the snippet below
(borrowed from LKMM docs). Here, P0 and P1 are 2 CPUs executing some code. P0
stores a message in <code>buf</code> and then signals to consumers like P1 that the
message is available &ndash; by doing a store to <code>flag</code>. P1 reads <code>flag</code> and if it
is set, knows that some data is available in <code>buf</code> and goes ahead and reads it.
However, if <code>flag</code> is not set, then P1 does nothing else. Without memory
barriers between P0&rsquo;s stores and P1&rsquo;s loads, the stores can appear out of order
to P1 (on some systems), thus breaking the pattern. The condition <code>r1 == 0 and r2 == 1</code> is a failure in the below code and would violate the condition. Only
after the <code>flag</code> variable is updated, should P1 be allowed to read the <code>buf</code>
(&ldquo;message&rdquo;).</p>
<pre tabindex="0"><code>        int buf = 0, flag = 0;

        P0()
        {
                WRITE_ONCE(buf, 1);
                WRITE_ONCE(flag, 1);
        }

        P1()
        {
                int r1;
                int r2 = 0;

                r1 = READ_ONCE(flag);
                if (r1)
                        r2 = READ_ONCE(buf);
        }
</code></pre><p>Below is a simple program in PlusCal to model the &ldquo;Message passing&rdquo; access
pattern and check whether the failure scenario <code>r1 == 0 and r2 == 1</code> could ever
occur. In PlusCal, we can model the non deterministic out-of-order stores to
<code>buf</code> and <code>flag</code> using an <code>either or</code> block. This makes PlusCal evaluate both
scenarios of stores (store to <code>buf</code> first and then <code>flag</code>, or viceversa) during
model checking. The technique used for modeling this non-determinism is similar
to how it is done in Promela/Spin using an &ldquo;if block&rdquo; (Refer to Paul McKenney&rsquo;s
perfbook for details on that).</p>
<pre tabindex="0"><code>EXTENDS Integers, TLC
(*--algorithm mp_pattern
variables
    buf = 0,
    flag = 0;

process Writer = 1
variables
    begin
e0:
       either
e1:        buf := 1;
e2:        flag := 1;
        or
e3:        flag := 1;
e4:        buf := 1;
        end either;
end process;

process Reader = 2
variables
    r1 = 0,
    r2 = 0;  
    begin
e5:     r1 := flag;
e6:     if r1 = 1 then
e7:         r2 := buf;
        end if;
e8:     assert r1 = 0 \/ r2 = 1;
end process;

end algorithm;*)
</code></pre><p>Sure enough, the <code>assert r1 = 0 \/ r2 = 1;</code>  fires when the PlusCal program is run through the TLC model checker.</p>
<p>I do find the <code>either or</code> block clunky, and wish I could just do something like:</p>
<pre tabindex="0"><code>non_deterministic {
        buf := 1;
        flag := 1;
}
</code></pre><p>And then, PlusCal should evaluate both store orders. In fact, if I wanted more than 2 stores, then it can get crazy pretty quickly without such a construct. I should try to hack the PlusCal sources soon if I get time, to do exactly this. Thankfully it is open source software.</p>
<p>Other notes:</p>
<ul>
<li>
<p>PlusCal is a powerful language that translates to TLA+. TLA+ is to PlusCal what assembler is to C. I do find PlusCal&rsquo;s syntax to be non-intuitive but that could just be because I am new to it. In particular, I hate having to mark statements with labels if I don&rsquo;t want them to atomically execute with neighboring statements. In PlusCal, a label is used to mark a statement as an &ldquo;atomic&rdquo; entity. A group of statements under a label are all atomic. However, if you don&rsquo;t specific labels on every statement like I did above (<code>eX</code>), then everything goes under a neighboring label. I wish PlusCal had an option, where a programmer could add implict labels to all statements, and then add explicit <code>atomic { }</code> blocks around statements that were indeed atomic. This is similar to how it is done in Promela/Spin.</p>
</li>
<li>
<p>I might try to hack up my own compiler to TLA+ if I can find the time to, or better yet modify PlusCal itself to do what I want. Thankfully the code for the PlusCal translator is open source software.</p>
</li>
</ul>
</div>

    </article>

    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">On workings of hrtimer&#39;s slack time functionality</h1>
    
    
      <p class="meta">
        
        
           | <a href="http://localhost:1313/blog/1/01/01/on-workings-of-hrtimers-slack-time-functionality/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/1/01/01/on-workings-of-hrtimers-slack-time-functionality/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>Below are some notes I wrote while studying hrtimer slack behavior (range
timers), which was added to reduce wakeups and save power, in the commit below.
The idea is that:</p>
<ol>
<li>Normal hrtimers will have both a soft and hard expiry which are equal to each other.</li>
<li>But hrtimers with timer slack will have a soft expiry and a hard expiry which is the soft expiry + delta.</li>
</ol>
<p>The slack/delay effect is achieved by splitting the execution of the timer
function, and the programming of the next timer event into 2 separate steps.
That is, we execute the timer function as soon as we notice that its soft
expiry has passed (<code>hrtimer_run_queues()</code>). However, for programming the next
timer interrupt, we only look at the hard expiry (<code>hrtimer_update_next_event()</code>
-&gt; <code>__hrtimer_get_next_event()</code> -&gt;
<code>__hrtimer_next_event_base()</code>-&gt;<code>hrtimer_get_expires()</code>). As a result, the only
way a slack-based timer will execute before its slack time elapses, is, if
another timer without any slack time gets queued such that it hard-expires
before the slack time of the slack-based timer passes.</p>
<p>The commit containing the original code added for range timers is:</p>
<pre tabindex="0"><code>commit 654c8e0b1c623b156c5b92f28d914ab38c9c2c90
Author: Arjan van de Ven &lt;arjan@linux.intel.com&gt;
Date:   Mon Sep 1 15:47:08 2008 -0700

    hrtimer: turn hrtimers into range timers
   
    this patch turns hrtimers into range timers;
    they have 2 expire points
    1) the soft expire point
    2) the hard expire point
   
    the kernel will do it&#39;s regular best effort attempt to get the timer run at
the hard expire point. However, if some other time fires after the soft expire
point, the kernel now has the freedom to fire this timer at this point, and
thus grouping the events and preventing a power-expensive wakeup in the future.
</code></pre><p>The original code seems a bit buggy. I got a bit confused about how/where we
handle the case in <code>hrtimer_interrupt()</code> where other normal timers that expire
before the slack time elapses, have their next timer interrupt programmed
correctly such that the interrupt goes off before the slack time passes.</p>
<p>To see the issue, consider the case where we have 2 timers queued:</p>
<ol>
<li>
<p>The first one soft expires at t = 10, and say it has a slack of 50, so it hard expires at t = 60.</p>
</li>
<li>
<p>The second one is a normal timer, so the soft/hard expiry of it is both at t = 30.</p>
</li>
</ol>
<p>Now say, an hrtimer interrupt happens at t=5 courtesy of an unrelated expiring
timer. In the below code, we notice that the next expiring timer is (the one
with slack one), which has not soft-expired yet. So we have no reason to run
it. However, we reprogram the next timer interrupt to be t=60 which is its hard
expiry time (this is stored in expires_next to use as the value to program the
next timer interrupt with).  Now we have a big problem, because the timer
expiring at t=30 will not run in time and run much later.</p>
<p>As shown below, the loop in <code>hrtimer_interrupt()</code> goes through all the active
timers in the timerqueue, <code>_softexpires</code> is made to be the real expiry, and the
old <code>_expires</code> now becomes <code>_softexpires + slack</code>.</p>
<pre tabindex="0"><code>       while((node = timerqueue_getnext(&amp;base-&gt;active))) {
              struct hrtimer *timer;

              timer = container_of(node, struct hrtimer, node);

              /*
               * The immediate goal for using the softexpires is
               * minimizing wakeups, not running timers at the
               * earliest interrupt after their soft expiration.
               * This allows us to avoid using a Priority Search
               * Tree, which can answer a stabbing querry for
               * overlapping intervals and instead use the simple
               * BST we already have.
               * We don&#39;t add extra wakeups by delaying timers that
               * are right-of a not yet expired timer, because that
               * timer will have to trigger a wakeup anyway.
               */

              if (basenow.tv64 &lt; hrtimer_get_softexpires_tv64(timer)) {
                      ktime_t expires;

                      expires = ktime_sub(hrtimer_get_expires(timer),
                                          base-&gt;offset);
                      if (expires.tv64 &lt; expires_next.tv64)
                              expires_next = expires;
                      break;
              }

              __run_hrtimer(timer, &amp;basenow);
      }
</code></pre><p>However, this seems to be an old kernel issue, as, in upstream v6.0, I believe
the next hrtimer interrupt will be programmed correctly because
<code>__hrtimer_next_event_base()</code> calls <code>hrtimer_get_expires()</code> which correctly use
the &ldquo;hard expiry&rdquo; times to do the programming.</p>
</div>

    </article>

    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">PowerPC stack guard false positives in Linux kernel</h1>
    
    
      <p class="meta">
        
        
           | <a href="http://localhost:1313/blog/1/01/01/powerpc-stack-guard-false-positives-in-linux-kernel/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/1/01/01/powerpc-stack-guard-false-positives-in-linux-kernel/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>Recently, the RCU mailing list
<a href="https://lore.kernel.org/rcu/CAABZP2xVCQhizytn4H9Co7OU3UCSb_qNJaOszOawUFpeo=qpWQ@mail.gmail.com/T/#t">received</a>
a report about an SRCU function failing stack guard checks.</p>
<p>Stack guard canaries are a security mechanism used to detect stack buffer
overflows. This mechanism works by placing a random value, called a canary,
between the local variables and the return address on the stack. If a buffer
overflow occurs, the canary value will be overwritten and the stack guard check
will fail, indicating that the program is being attacked. False positives can
occur if the canary value is overwritten by a legitimate write operation, such
as when a large structure is copied onto the stack.</p>
<p>Closer inspection of the function (<code>srcu_gp_start_if_needed</code>) did not reveal
any buffers that may be overflowed.</p>
<p>After discussions with a number of kernel developers, it is clear what the
issue is. Firstly, credit to Boqun Feng for looking through disassembly and
pointing things out which led to the whole email chain and discovery of the
issue.</p>
<p>A significant hint came from Christophe who is the kernel author of PPC’s stack
protection, he mentioned:</p>
<pre tabindex="0"><code>Each task has its own canary, stored in task struct :

kernel/fork.c:1012:
tsk-&gt;stack_canary = get_random_canary();

On PPC32 we have register &#39;r2&#39; that points to task struct at all time, 
so GCC is instructed to find canary at an offset from r2.
But on PPC64 we have no such register.

Instead we have r13 that points to the PACA struct which is a per-cpu
structure, and we have a pointer to &#39;current&#39; task struct in the PACA struct.
So in order to be able to have the canary as an offset of a fixed register as
expected by GCC, we copy the task canary into the cpu&#39;s PACA struct during
_switch():

	addi	r6,r4,-THREAD	/* Convert THREAD to &#39;current&#39; */
	std	r6,PACACURRENT(r13)	/* Set new &#39;current&#39; */
  #if defined(CONFIG_STACKPROTECTOR)
	  ld	r6, TASK_CANARY(r6)
	  std	r6, PACA_CANARY(r13)
  #endif
</code></pre><hr>
<p>In 64-bit PPC, the TLS (Thread Local Storage) cannot be pointed to by register <code>r2</code> as it is used to store the TOC (Table of Contents) pointer instead, which is used for accessing global and static data. Therefore, the kernel saves the currently running <code>task_struct</code> in the per-CPU area pointed to by <code>r13</code> instead. This is known as the Processor Access Control Area (PACA). The PACA is a per-CPU memory area that stores information about the CPU&rsquo;s context. One of the users of this data structure is to save the current instruction location prior to interrupt processing.</p>
<p>On PPC64, each task has its own stack canary, stored in the task struct. However, unlike PPC32, there is no fixed register that points to the currently running <code>task_struct</code> at all times. Instead, the per-CPU PACA struct contains a pointer to the current <code>task_struct</code>. Therefore, in order to be able to have the canary as an offset of a fixed register as expected by GCC, the task canary is copied into the PACA struct during <code>_switch()</code>. False positives can occur if GCC keeps an old value of the per-CPU struct pointer, which then gets the canary from the wrong CPU struct, leading to a different task.</p>
<p>This issue with storing canaries of the currently running task is related to the issue of not being able to use <code>r2</code> to point to the TLS on 64-bit PPC. The kernel must use the per-CPU area to store the currently running <code>task_struct</code>, which leads to the need to copy the task canary into the PACA struct during <code>_switch()</code>. The compiler optimization that causes the register <code>r13</code> to be cached into <code>r10</code> can then lead to false positives if GCC keeps an old value of the per-CPU struct pointer.</p>
<h3 id="so-what-the-heck-is-paca">So what the heck is PACA?</h3>
<p>This <code>r13</code> register points to a structure in the kernel called PACA which is a per-CPU memory area storing information about the CPU’s context.</p>
<p>Per the PPC64 <a href="https://www.kernel.org/doc/ols/2001/ppc64.pdf">paper</a>:</p>
<p>This structure contains information unique to each processor; therefore an array of PACAs are created, one for each logical processor. One of the users of this data structure to save the current instruction location prior to interrupt processing.</p>
<h3 id="so-whats-up-with-the-canary">So what’s up with the Canary?</h3>
<p>As explained earlier in the article and as Christpophe answered this for me:</p>
<pre tabindex="0"><code>PPC64 uses a per-task canary. But unlike PPC32, PPC64 doesn&#39;t have a fixed 
register pointing to &#39;current&#39; at all time so the canary is copied into 
a per-cpu struct (PACA) during _switch().

If GCC keeps an old value of the per-cpu struct pointer, it then gets 
the canary from the wrong CPU struct so from a different task.
</code></pre><h3 id="compiler-optimizations">Compiler optimizations</h3>
<p>What Christophe refers to in the last line is exactly a Compilter optimization. It turns out that from the reporter’s email, the <code>r10</code> register was used as a base pointer to the per-CPU PACA area. This means the compiler must have cached <code>r13</code> into <code>r10</code>, perhaps because it wanted to use <code>r13</code> for something else. Boqun Feng provided the following snippet which Zhouyi verified fixes the issue:</p>
<pre tabindex="0"><code>diff --git a/kernel/rcu/srcutree.c b/kernel/rcu/srcutree.c
        index ab4ee58af84b..f5ae3be3d04d 100644
        --- a/kernel/rcu/srcutree.c
        +++ b/kernel/rcu/srcutree.c
        @@ -747,6 +747,7 @@ void __srcu_read_unlock_nmisafe(struct srcu_struct *ssp, int idx)

                smp_mb__before_atomic(); /* C */  /* Avoid leaking the critical section. */
                atomic_long_inc(&amp;sdp-&gt;srcu_unlock_count[idx]);
        +       asm volatile(&#34;&#34; : : : &#34;r13&#34;, &#34;memory&#34;);
         }
         EXPORT_SYMBOL_GPL(__srcu_read_unlock_nmisafe);
</code></pre><p>In this snippet, <code>r13</code> is added to an extended inline asm statement, which instructs the compiler that <code>r13</code> may be clobbered by the asm statement, which hopefully prevents the compiler from caching its value before exiting the function. In fact this is exactly what prevents the issue.</p>
<p>Boqun also included a <code>memory</code> clobber which is equivalent to <code>barrier()</code> and is additional step which hopefully ensures memory access compiler optimizations don’t span the inline assembly statement.</p>
<h3 id="finally-the-issue-is-clear">Finally the issue is clear</h3>
<p>Later in the email chain, I mentioned the series of events as outlined by Christophe and Boqun which could lead to the issue:</p>
<pre tabindex="0"><code>The issue requires the following ingredients:
1. Task A is running on CPU 1, and the task&#39;s canary is copied into
the CPU1&#39;s per-cpu area pointed to by r13.
2. r13 is now cached into r10 in the offending function due to the compiler.
3. Task A running on CPU 1 now gets preempted right in the middle of
the offending SRCU function and gets migrated to CPU 2.
4.  CPU 2&#39;s per-cpu canary is updated to that of task A since task A
is the current task now.
5. Task B now runs on CPU 1 and the per-cpu canary on CPU 1 is now that of B.
6. Task A exits the function, but stack checking code reads r10 which
contains CPU 1&#39;s canary which is that of task B!
7. Boom.

So the issue is precisely in #2.  The issue is in the compiler that it
does not treat r13 as volatile as Boqun had initially mentioned.
</code></pre><h2 id="how-do-we-fix-it">How do we fix it?</h2>
<p>My / our current take on it is it appears to be a compiler bug where the register <code>r13</code> is not considered volatile (which works for user land but not for the kernel). Seher Boessenkool who has worked on similar PPC64 issues before is on the email chain and can hopefully fix it in the compiler but lets see where it goes.</p>
<p>As a quick hack to fix this (as shown by Boqun above),<code>r13</code> can be added to an extended inline asm statement, which instructs the compiler that <code>r13</code> may be clobbered by the asm statement, hopefully preventing the compiler from caching its value before exiting the function.</p>
<h2 id="can-we-fix-it-in-the-kernel">Can we fix it in the kernel?</h2>
<p>According to Michael Ellerman, a possible solution would be to keep current in a register (GPR) on 64-bit, but we&rsquo;d need to do that in addition to the register reserved for the PACA, so that would consume another GPR which we&rsquo;d need to think hard about.</p>
<p>There&rsquo;s another reason to have the canary in the PACA, according to him: The PACA is always accessible, even when the MMU is off (because it is in a register), whereas <code>current</code> isn&rsquo;t (in some situations).</p>
<p>Even though, we prefer not to use stack protector in code that runs with the MMU off — if the canary wasn&rsquo;t in the PACA to begin with, then we&rsquo;d have a hard requirement to not use stack protector in code paths where the MMU is off.</p>
</div>

    </article>

    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">RCU and dynticks-idle mode</h1>
    
    
      <p class="meta">
        
        
           | <a href="http://localhost:1313/blog/1/01/01/rcu-and-dynticks-idle-mode/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/1/01/01/rcu-and-dynticks-idle-mode/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>Note 1: RCU is an extremely complex topic and I make no claims of accuracy,
correctness and don&rsquo;t make any claims that this document is to be used as a
defacto reference for any purpose. You have been warned! For more accurate and
standard references, I will refer you to the kernel RCU documentation.  Please
consider this post as rough notes. That said, your corrections, and comments are
welcomed.</p>
<p>Note 2: The article is a WIP and not fully finished (thought it is almost).</p>
<p>The kernel&rsquo;s <code>dynticks-idle</code> mode is a mode of a CPU in which the CPU is idle
and the scheduler clock tick has been turned off to save power and let the CPU
to continue to be in lower power state for a long time. Also known as NO_HZ.</p>
<p>A CPU in this mode presents some challenges to RCU. This is because an RCU
grace period completion depends on RCU knowing that a CPU has transitioned
through a quiescent state. When the CPU is idle but the scheduling clock tick
is not turned off, RCU on that idle-but-ticking-CPU can simply report from the
tick path that the CPU is in a quiescent state. However in dynticks-idle mode
this isn&rsquo;t possible, so something more clever is needed. The same complications
arise due to the turning off of the tick in user mode (adaptive-ticks support).
This article goes through the design of RCU from this perspective.</p>
<p>For RCU&rsquo;s purposes, the kernel maintains a per-cpu datastructure called
<code>rcu_dynticks</code> which does this dynticks-idle state tracking.</p>
<h2 id="extended-quiescent-state-eqs">Extended Quiescent State (EQS)</h2>
<p>An extended quiescent state is defined as a processor state in which RCU
considers the CPU as not something that is using RCU. This is also important
for a more aggressive form of dynticks-idle code (CONFIG_NO_HZ_FULL) which not
only turns off the tick in the idle path but also in userspace if there is no
other need for the tick other than RCU (for example if only 1 task is running).</p>
<p>By defining certain contexts as an EQS, RCU will work no matter how aggressive
the dynticks-idle implementation.</p>
<p>AFAICT, there are 2 EQS states: dynticks-idle and usermode. In both these states,
the tick may be turned off and the CPU is considered to be in a quiescent state
and RCU is considered &ldquo;idle&rdquo;.</p>
<h2 id="entry-and-exit-into-an-eqs-due-to-transition-tofrom-non-idle-kernel-process-context">Entry and exit into an EQS due to transition to/from non-idle kernel process context</h2>
<p>The <code>rdtp-&gt;dynticks_nesting</code> counter tracks entry and exit into an EQS due to
transition from idle to process context or from usermode to process context.  A
value of 0 indicates that the CPU in an EQS and a value of &gt; 0 indicates that
it is not. A non-zero value also means we transitioned into the kernel&rsquo;s
non-idle process context.</p>
<p>An EQS can also be exited due to interrupt or NMI entry, but this doesn&rsquo;t
really track that. We&rsquo;ll talk about tracking those later.</p>
<p>A note about dynticks counters: In general the dynticks counters track the
number of reasons why we&rsquo;re not in an EQS (that is RCU is not &ldquo;idle&rdquo;). For
example, a value of zero thus means we ARE in an EQS. The
<code>rdtp-&gt;dynticks_nesting</code> counter tracks the number of process-level (non-idle
kernel process context)-level reasons why RCU is non-idle.</p>
<p>When I traced <code>rdtp-&gt;dynticks_nesting</code>, I could only find its value to be
either a 0 or a 1. However looking back at <a href="https://elixir.bootlin.com/linux/v3.19.8/source/kernel/rcu/rcu.h#L33">old kernel
sources</a>,
it appears that these can be nested becaues of so called &ldquo;half-interrupts&rdquo;. I
believe these are basically interrupts that cause a transition to usermode due
to usermode upcalls (usermode helper subsystem).
So a nesting situation could be something like: 1. Transition from idle to
process context which makes dynticks_nesting == 1. Next, an interrupt comes in
which makes a usermode upcall. This usermode call now makes a system call
causing entry back into process context, which increments the dynticks_nesting
counter to 2. Such a crazy situation is perhaps possible.</p>
<h2 id="another-way-some-paths-see-if-we-are-in-an-eqs-or-not">Another way some paths see if we are in an EQS or not</h2>
<p>The <code>rdtp-&gt;dynticks</code> counter is used to track transitions to/from dyntick-idle
mode. But it also can share light on whether we are in an EQS or not. If this
counter is odd, it means we are NOT in an EQS and if its even, then we ARE.</p>
<p>Note: since an EQS entry can happen even because of transition into usermode,
this counter is not only incremented due to entry into dyntick-idle mode, but
also due to transition into usermode. This is observed by seeing that an
increment of this counter can also happen due to
<code>rcu_user_enter</code>-&gt;<code>rcu_eqs_enter</code>-&gt;<code>rcu_dynticks_eqs_enter</code>.</p>
<p>The following function checks this:</p>
<pre tabindex="0"><code>/*
 * Is the current CPU in an extended quiescent state?
 *
 * No ordering, as we are sampling CPU-local information.
 */
bool rcu_dynticks_curr_cpu_in_eqs(void)
{
        struct rcu_dynticks *rdtp = this_cpu_ptr(&amp;rcu_dynticks);

        return !(atomic_read(&amp;rdtp-&gt;dynticks) &amp; RCU_DYNTICK_CTRL_CTR);
}
</code></pre><p>Any time the rdtp-&gt;dynticks counter&rsquo;s second-lowest most bit is not set, we are
in an EQS, and if its set, then we are not (second lowest because lowest is
reserved for something else as of v4.18-rc1). This function is not useful to
check if we&rsquo;re in an EQS from a timer tick though, because its possible the
timer tick interrupt entry caused an EQS exit which updated the counter. IOW,
the &lsquo;dynticks&rsquo; counter is not capable of checking if we had already exited the
EQS before. To check if we were in an EQS or not from the timer tick, we
instead must use <code>dynticks_nesting</code> counter. More on that later. The above
function is probably just useful to make sure that interrupt entry/exit is
properly updating the dynticks counter, and also to make sure from
non-interrupt context that RCU is in an EQS (see <code>rcu_gp_fqs</code> function).</p>
<h2 id="entry-and-exit-into-an-eqs-due-to-interrupts">Entry and exit into an EQS due to interrupts</h2>
<p>Other than the  entry/exit into usermode or idle, interrupts and NMIs can cause
the CPU to enter/exit a QS. Naturally, RCU needs to be &ldquo;watching&rdquo; as RCU
read-side critical sections are permitted in interrupt handlers so an exit from
an EQS for this purpose is a must. This is done by calls to
<code>rcu_eqs_enter/exit</code> from <code>rcu_irq_exit/enter</code> respectively.</p>
<p>The interrupt nesting level is also carefully tracked in
<code>rdtp-&gt;dynticks_nmi_nesting</code> as of v4.18-rc1, and we&rsquo;ll see later why this is
needed (reporting of a QS from the timer tick) and complications due to nested
NMIs (yes NMIs can nest!) that need to be handled. Both IRQ-nesting and
NMI-nesting use the same <code>dynticks_nmi_nesting</code> counter. More on this in the
&ldquo;Nested Interrupt Handling&rdquo; section.</p>
<p>With this knowledge in mind, lets discuss how a QS is reported from the tick
path when the tick is infact not turned off.</p>
<h2 id="how-are-qs-reported-from-the-timer-tick">How are QS reported from the timer tick</h2>
<p>As for 4.18-rc1, the tick call graph which checks for QS is as follows:</p>
<pre tabindex="0"><code>tick_sched_timer-&gt;
    tick_sched_handle-&gt;
	update_process_times
		rcu_check_callbacks
</code></pre><p>There are 3 variants of RCU (sched, bh and preempt). All these variants have
different ways of detecting a QS. Lets only talk about the checks for the
reporting of the sched RCU variant which is sufficient for the purposes of this
article.</p>
<p>For the sched RCU variant, we are in a QS if the CPU is either idle, or in
usermode. This awfully sounds like the definition of an EQS. However, we can&rsquo;t
use dynticks eqs detection (<code>rcu_dynticks_curr_cpu_in_eqs</code> mentioned earlier in
the article) because <code>rdtp-&gt;dynticks</code> is just a simple counter. Its has
evenness when we&rsquo;re in an EQS and oddity when we&rsquo;re not. It tells us nothing
about interrupt nesting. More on this in the below note.</p>
<p>Note: The timer tick path is itself triggered through an interrupt, so we
can&rsquo;t rely on the <code>rcu_dynticks_curr_cpu_in_eqs</code> detection to tell us if we&rsquo;re
in a QS or not. Instead we rely on other methods. First of all
<code>rcu_check_callbacks</code> is passed a user boolean parameter, which tells us if the
callback checking (tick) happened during usermode execution. So if that&rsquo;s the
case, its easy, we simply report the CPU to in a QS for rcu-sched. But what are
the other ways we could be in a QS? Just one more: If we were in the idle-loop
at the time of the <code>rcu_check_callbacks</code> getting called, AND  we&rsquo;re a 1st level
interrupt that caused a call to rcu_check_callbacks. This first level is infact
most likely the timer tick interrupt. The &ldquo;first level nesting check&rdquo; is
important, because only the outer most interrupt that interrupted the idle loop
should report the sched-QS. Any nested interrupts in the idle loop that cause
<code>rcu_check_callbacks</code> to be called (I don&rsquo;t know of any) should not report the
QS again. This interrupt nesting level is determined by <code>dynticks_nmi_nesting</code>
mentioned in earlier sections!</p>
<p>Turns out that these above checks (user or interrupt-from-idle) are also
worthwhile causes to report a bh and tasks RCU qs so we report them as such.</p>
<h2 id="nested-interrupt-and-nmi-handling">Nested Interrupt and NMI Handling</h2>
<p>During handling of nested interrupts, the <code>rcu-&gt;dynticks</code> counter which counts
CPU transitions through dynticks-idle or user mode should correctly maintain
the invariant: If its even, we&rsquo;re in an EQS and if its odd, we&rsquo;re not.</p>
<p>A (naive) algorithm may do something like:</p>
<pre tabindex="0"><code>void rcu_nmi_enter(void)
{
	if(dynticks_is_even())
		dynticks++;

	dynticks_nmi_nesting++;
}

void rcu_nmi_exit(void)
{
	if (dynticks_nmi_nesting != 1) {
		dynticks_nmi_nesting--;
		return;
	}

	dynticks_nmi_nesting = 0;
	dynticks++;
}
</code></pre><p>The problem with this algorithm is if you have an NMI come in while
rcu_nmi_enter is running, bad things can happen. Specifically, the inner
rcu_nmi_enter/exit pair, can result in premature exit from an EQS state.</p>
<p>To see this let us take the case where an NMI comes in before dynticks is
incremented in the outer rcu_nmi_enter. In this case nothing bad will happen.
But say the NMI comes in after dynticks is incremented in the outer
<code>rcu_nmi_enter</code> but before <code>dynticks_nmi_nesting</code> is incremented. Then what will
happen is:</p>
<p>The steps would look like:</p>
<ol>
<li>The outer rcu_nmi_enter will update dynticks to be odd.</li>
<li>An NMI comes in after dynticks is made odd by dynticks++, but before dynticks_nmi_nesting is updated.</li>
<li>The second <code>rcu_nmi_enter</code> comes in and it will leave <code>dynticks</code> alone but increase <code>dynticks_nmi_nesting</code> to 1.</li>
<li>Now on the corresponding inner <code>rcu_nmi_exit</code>, it will notice
<code>dynticks_nmi_nesting</code> is 1 so it will set it to 0.</li>
<li>Next it will wrongly increment <code>dynticks</code> messing it up completely. The
inner rcu_nmi_exit is never supposed to exit EQS. Only the outer one is.</li>
</ol>
<p>The problem here is the inner <code>rcu_nmi_exit</code> increments the dynticks counter
(thus marking the dynticks-idle mode as exited even though we&rsquo;re still in the
inner nested interrupt!) but there&rsquo;s no way of knowing not to do that because
the outer <code>rcu_nmi_enter</code> hasn&rsquo;t incremented <code>dynticks_nmi_nesting</code> yet!</p>
<p>The desired behavior is, because the outer <code>rcu_nmi_enter</code> exited dynticks-idle
mode (incremented dynticks to odd), only the outer <code>rcu_nmi_exit</code> should make
it even (and mark an entry back into dynticks-idle mode).</p>
<p>The fix is an algorithm like the following <a href="http://lkml.kernel.org/r/CALCETrXSY9JpW3uE6H8WYk81sg56qasA2aqmjMPsq5dOtzso=g@mail.gmail.com">proposed by Andy Luto</a> and <a href="https://lkml.kernel.org/r/20141122234157.GB5050@linux.vnet.ibm.com">formally written and verified by Paul</a>:</p>
<pre tabindex="0"><code>void rcu_nmi_enter(void)
{
	int incby = 2;

	if(dynticks_is_even()) {
		incby = 1;
		dynticks++;
	}

	dynticks_nmi_nesting += incby;
}

void rcu_nmi_exit(void)
{
	if (dynticks_nmi_nesting != 1) {
		dynticks_nmi_nesting -= 2;
		return;
	}

	dynticks_nmi_nesting = 0;
	dynticks++;
}
</code></pre><p>The GOOD steps would now be:</p>
<ol>
<li>The outer rcu_nmi_enter will update dynticks to be odd and set local
variable incby to 1.</li>
<li>An NMI comes in after dynticks is made odd by dynticks++, but before
dynticks_nmi_nesting is increased by incby.</li>
<li>The second <code>rcu_nmi_enter</code> comes in and it will leave <code>dynticks</code> alone but
increase <code>dynticks_nmi_nesting</code> to 2 (incby is 2 if dynticks was left
alone).</li>
<li>Now on the corresponding inner <code>rcu_nmi_exit</code>, it will notice
<code>dynticks_nmi_nesting</code> is not 1, so it will set it decrease nmi_nesting to 0
and return WITHOUT messing up the <code>dynticks</code> counter.</li>
<li>The outer <code>rcu_nmi_enter</code> now finally does increase <code>dynticks_nmi_nesting</code>
by 1.</li>
<li>The outer <code>rcu_nmi_exit</code> will now set <code>dynticks_nmi_nesting</code> to 0 and do the
<code>dynticks++</code> causing an entry back into dynticks-idle mode.</li>
</ol>
<h2 id="handling-of-usermode-upcalls-from-interrupts">Handling of usermode upcalls from interrupts</h2>
<p>RCU&rsquo;s design tries to handle conditions where a usermode upcall was made from
IRQ context, with the IRQ entry never being matched with an IRQ exit! These are
so called &ldquo;half interrupts&rdquo;. Due to this, the rcu_nmi_nesting counter can go
out sync because an rcu_irq_enter will not be paired properly with an
rcu_irq_exit.</p>
<p>This is the reason for a separate <code>dynticks_nmi_nesting</code> counter and a
<code>dynticks_nesting</code> counter. Special &ldquo;fixing up&rdquo; of the dynticks_nmi_nesting is
done to make sure this counter is sane. See next paragraphs on the fixup info.</p>
<p>When dynticks_nesting is decremented to 0 (the outermost process-context
nesting level exit causes an eqs-entry), the dynticks_nmi_nesting is reset to
0. This makes sense because we&rsquo;re no longer in an NMI at this point.</p>
<p>Similarly, when the dynticks_nesting is set to 1, we have entered a
process-context and dynticks_nmi_nesting is set to a high value. This is also
Ok because the dynticks_nmi_nesting serves no purpose (RCU has already exited
the EQS state).</p>
<h2 id="conclusion">Conclusion</h2>
<p>RCU has to watch over what&rsquo;s happening in the system carefully. This makes the
subsystem complex and requires it to handle various weird usages such as
half-interrupts and nested NMIs. The need to save power via dynticks-idle and
adaptive-ticks modes further complicates RCU. Hopefully this article sheds some
light on the foundation blocks of this dynticks RCU tracking which is the basis
of things happening in other areas such as forcing of quiescent states (fqs).</p>
</div>

    </article>

    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">RCU-preempt: What happens on a context switch</h1>
    
    
      <p class="meta">
        
        
           | <a href="http://localhost:1313/blog/1/01/01/rcu-preempt-what-happens-on-a-context-switch/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/1/01/01/rcu-preempt-what-happens-on-a-context-switch/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>Note: This article requires knowledge of RCU (read copy update) basics and its
different flavors.</p>
<p>RCU&rsquo;s main algorithm is to detect when it is free to reclaim objects that RCU
readers no longer need. The &ldquo;RCU-sched&rdquo; flavor of RCU does this by just
disabling preemption across the read section. So any time any of the CPUs is
not running in a preempt disabled section (such as with preemption off, or
interrupts off), then the CPU is said to be in a &ldquo;quiescent state&rdquo; (QS). Once
all CPUs reach a QS after the reclaimer filed a claim to release an object, the
object can be safely released. The time from when the request for RCU to
release an object to when RCU says its Ok to release it, is called the grace
period.</p>
<p>RCU-sched is kind of a big hammer, having readers disable preemption can have
poor performance effects. After all, read sections are expected to be light in
RCU. It can also effect real-time response of applications.</p>
<p>For this reason, preemptible RCU came about (also called RCU-preempt).
Obviously in this flavor, RCU reader sections can get preempted to run
something else.</p>
<p>A <a href="https://www.mail-archive.com/linux-kernel@vger.kernel.org/msg1682346.html">recent discussion</a>
on LKML clarified to me that &ldquo;preempted to run something else&rdquo; not only covers
involuntary preemption but also voluntarily sleeping. This design is because,
with <code>PREEMPT_RT</code> kernels, &ldquo;rt&rdquo; version of spinlocks are actually mutexes that
can put the RCU reader to sleep.</p>
<p>So coming back to the point of this article, I want to go over what happens on
a context-switch. When the scheduler is called, we end up in <code>__schedule</code>
function. Here in the beginning <code>rcu_note_context_switch</code> is called with the
<code>preempt</code> parameter. The <code>preempt</code> parameter indicates if task blocked with
help of <code>schedule()</code> or if it was a kernel path (such as return from interrupts
or system calls) that called into the scheduler to preempt the currently
running task.</p>
<p><code>rcu_note_context_switch</code> first calls <code>rcu_preempt_note_context_switch</code> for
RCU-preempt to take note. Lets discuss this function.</p>
<p>First note that the RCU-preempt flavor does warn you if you voluntarily sleep inside
an RCU read side section. I&rsquo;m not sure how the &ldquo;RT-spinlock&rdquo; for RT kernels
doesn&rsquo;t get this warning. Probably they delete this warning in PREEMPT_RT
patchset, idk. The warning is <code>WARN_ON_ONCE(!preempt &amp;&amp; t-&gt;rcu_read_lock_nesting &gt; 0);</code>. But seems pretty clear to me a non-RT kernel
would scream with this warning if an RCU-preempt read section went to sleep.
Getting preempted is Ok but not voluntary sleeping according to this code! (see
side note in last para)</p>
<p>If the task being preempted is in a read-side RCU section, then (and only then)
it calls <code>rcu_preempt_ctxt_queue</code>. Here the task being preempted is added to a
list of blocked tasks. The reason why we need to add it is, RCU-preempt has 2
perspectives of Quiescent state (QS). Recall, a QS is reached whenever an
entity is not blocking the current grace period (GP). RCU-preempt considers 2
entity perspectives: Either the task, or the CPU. In the RCU-preempt world, if
a task that is currently in an RCU read section gets preempted, then the CPU
has reached a QS because it is no longer running the RCU-read section that is
blocking the GP. But now, the task has reached a non-QS (It is blocking the
GP). This list basically indicates this fact. If there are blocking tasks, then
the GP cannot complete even though the CPU reports its QS. <a href="https://lkml.org/lkml/2018/5/4/632">Paul Mckenney explains this here</a>. The other benefit of having a list of tasks is that preempted RCU read sections can be boosted. Paul Mckenney again came to the rescue to <a href="https://lkml.org/lkml/2018/5/4/659">explain this to me</a>.</p>
<p>Finally, you see that <code>rcu_preempt_note_context_switch</code> does report a QS. This
is because if the task was in a read section, it has just been added to the
blocked task list. If its not, then we just reached a QS for the CPU. Either
way we entered a CPU QS. So is recorded with a call to <code>rcu_preempt_qs();</code>.</p>
<p>Please go through the <a href="https://www.kernel.org/doc/Documentation/RCU/Design/Expedited-Grace-Periods/Expedited-Grace-Periods.html">Expedited GP document</a> which also explains some of the RCU-preempt behaviors.</p>
<p>Side note: At the moment, I don&rsquo;t immediately see why by blocking in a RCU-preempt
section shouldn&rsquo;t be allowed.  Since we&rsquo;re tracking blocked tasks the same way
as preempted tasks, it should be possible to handle them the same way. They
both cause a CPU QS and a task non-QS to be entered, they both need priority
boosting. Perhaps the warning should be removed? Let me know your feedback in
the comments.</p>
</div>

    </article>

    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">Resources</h1>
    
    
      <p class="meta">
        
        
           | <a href="http://localhost:1313/resources/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/resources/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><h2 id="conference-presentations">Conference Presentations</h2>
<ul>
<li>
<p>Make RCU do less to save power (Lazy RCU). (Linux Plumbers Conference,
September 2023). <a href="/resources/lpc2022-rcu-do-less.pdf">link</a></p>
</li>
<li>
<p>Core Scheduling: Taming Hyper-Threads to be secure (Linux Plumbers Conference, August 2020).</p>
</li>
<li>
<p>RCU in 2019: KernelRecipes 2019 (Paris, 09/19):
<a href="/resources/RCU_in_2019_KernelRecipes.pdf">slides</a>, <a href="https://youtu.be/dvtZsBlSECk">video (third
speaker)</a>.</p>
</li>
<li>
<p>Debugging Real-Time issues in Linux. Conference: ELCE 2016 (Berlin, 10/16): :
<a href="/resources/elce2016-debug-rt.pdf">slides</a>,
<a href="https://s3.amazonaws.com/connect.linaro.org/yvr18/videos/yvr18-pmw10.mp4">video</a>.
Also presented at Embedded Systems Conference (ESC) Silicon Valley (12/16).</p>
</li>
<li>
<p>adeb: The better adb shell. Conference: Linaro Connect 2018 (Vancouver,
09/18). <a href="/resources/adeb-lc18.pdf">slides</a>.</p>
</li>
<li>
<p>BPFd: Powerful Linux Tracing for Remote targets using eBPF. Conference:
SCALE16x (Pasadena, 3/18):  <a href="/resources/bcc-scale.pdf">slides</a>,
<a href="https://www.youtube.com/watch?v=bPrY3ZKvQfM">video</a>,
<a href="https://lwn.net/Articles/744522/">article</a>. Also at OS Power Management (OSPM)
conference on 4/18 (Italy).</p>
</li>
<li>
<p>Flattened Image Tree (FIT). Conference: ELC 2013 (Sanfrancisco, USA 12/13): <a href="/resources/FIT-talk.pdf">slides</a> <a href="https://www.youtube.com/watch?v=cVSEfOfb6rs">video</a>.</p>
</li>
</ul>
<h2 id="recently-published-work">Recently Published Work</h2>
<ul>
<li>
<p>RCU Usage In the Linux Kernel: Eighteen Years Later, ACM SIGOPS Operating
Systems Review, August 2020. (Latest <a href="/resources/rcu_usage_09_2020.pdf">version in PDF</a>).</p>
</li>
<li>
<p>Full and efficient protection of kernel mode from hyper-threading attacks,
August 2020. Defensively published: IDF-304986. <a href="/resources/defensive_pub_ht.pdf">link</a>.</p>
</li>
<li>
<p>Improving ChromeOS performance with core scheduling (I am primary author). Published on ChromeOS blog: <a href="https://chromeos.dev/en/posts/improving-chromeos-performance-with-core-scheduling">link</a>.</p>
</li>
<li>
<p>Garbage Collection using Userfaultfd (Lokesh Gidra, Joel Fernandes, Hans
Boehm). Defensively published: IDF-300892.</p>
</li>
<li>
<p>Energy-Efficient Low-latency Audio on Android. Journal of Systems and
Software Volume 152, June 2019, Pages 182-195 ELSEVIER. <a href="https://www.sciencedirect.com/science/article/pii/S0164121219300585">link</a></p>
</li>
<li>
<p>LWN Journal (Linux Weekly News). BPFd: Running BCC tools remotely across systems and architectures. <a href="https://lwn.net/Articles/744522/">publication link</a></p>
</li>
<li>
<p>The In-circuit SD card switch, published on Hackaday. <a href="http://hackaday.com/2014/06/08/the-in-circuit-sd-card-switch/">link</a>.</p>
</li>
</ul>
</div>

    </article>

    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">SELinux Debugging on ChromeOS</h1>
    
    
      <p class="meta">
        
        
           | <a href="http://localhost:1313/blog/1/01/01/selinux-debugging-on-chromeos/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/1/01/01/selinux-debugging-on-chromeos/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>Not being an SELinux expert but having to deal with it from time-to-time, I find it may be productive to write some debugging notes. So this post is about an issue that I fixed just this morning.</p>
<h2 id="issue-runtime-labeled-pseudo-files-are-not-getting-labelled">Issue: Runtime labeled pseudo-files are not getting labelled</h2>
<p>On ChromeOS, there are Android-specific policies which are combined with the ChromeOS ones. For someone who tries to stay away from SELinux, this is pretty much voodoo.</p>
<p>Let me start with the last issue. I had to label a procfs file added by a test kernel patch which I am developed. This procfs would then be toggled by ChromeOS’s experiment system to enable the feature and collect data. The procfs file was called <code>/proc/sys/kernel/timer_highres</code>.</p>
<p>ChromeOS’s SELinux policies live in a repository at <code>src/platform2/sepolicy</code> as of this writing. In this repo, there is a file named<code>genfs_contexts</code> which contains <code>genfscon</code> rules.</p>
<p><code>genfscon</code> is used to dynamically generate security contexts for pseudo-filesystems such as <code>procfs</code>. This is documented more in the <a href="https://selinuxproject.org/page/FileStatements#File_System_Labeling_Statements">docs here</a>.</p>
<p>The main issue here was I was making changes to the wrong <code>genfs_contexts</code> file to begin with. That’s easy to do when there are 2 of them. But here’s how I found out that my changes were not getting through to the device:</p>
<ol>
<li>Make <a href="https://chromium-review.googlesource.com/c/chromiumos/platform2/+/4568967/1/sepolicy/policy/base/genfs_contexts">the change</a> to <code>genfs_contexts</code> file to label the proc node I am interested in.</li>
<li>After building and installing the <code>platform-base/selinux-policy</code> ChromeOS package, inspect the device.</li>
<li>Where to inspect? Read the <a href="https://source.chromium.org/chromiumos/chromiumos/codesearch/+/main:src/third_party/chromiumos-overlay/chromeos-base/selinux-policy/selinux-policy-9999.ebuild;l=312?q=selinux-policy%20ebuild">eBuild file sources</a> which is what builds that package. From this I figured that the <code>genfs_contexts</code> was being compiled into a binary located at <code>/etc/selinux/arc/policy.30</code>. I am still not sure what the numeric suffix means, but it does not matter.</li>
<li>Next, use <code>grep</code> to scan this binary for <code>timer_highres</code>, I found that the <code>grep</code> brought back nothing. Confirming that my change had no effect.</li>
<li>Hmm, what if the string was actually some weird encoding and <code>grep</code> could not find it? After all, I know nothing about that binary file’s format.
<ol>
<li>So copy policy binary file and confirm with <code>seinfo</code> command on a regular Linux machine:</li>
<li>First <code>scp</code> the <code>policy.30</code> file from the device.</li>
<li>Then on regular Linux, run: <code>seinfo --genfscon=proc policy.30 | less</code>.</li>
<li>Look for the <code>genfscon</code> line for <code>timer_highres</code> in the output.</li>
</ol>
</li>
<li><a href="https://chromium-review.googlesource.com/c/chromiumos/platform2/+/4606196">Modify</a> the correct <code>genfs_contexts</code> file and repeat.</li>
<li>Use <code>ls -lZ</code> to confirm that the procfs file is now correctly labeled.</li>
</ol>
<h2 id="conclusions-and-lessons-learnt">Conclusions and Lessons Learnt</h2>
<ol>
<li>Trial and error approach to debugging is OK, but always make sure your changes are being reflected on what is on the device, otherwise you’ll waste a lot of time.</li>
<li>Once a theory is validated, example: “<code>genfscon</code> rule did not get updated”, then stop with step #1, and focus on digging deeper into the validated theory which we know is de-facto correct.</li>
<li>Read the build source files to get a better understanding of how your code changes result in the artifacts. That’s how I learnt the <code>genfscon</code> rule I was adding was getting compiled into the <code>policy.30</code> file.</li>
<li>Document the issue like in this blog post, and also in the source file being changed so that others don’t run into the issue in the future.</li>
</ol>
</div>

    </article>

    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">Single-stepping the kernel&#39;s C code</h1>
    
    
      <p class="meta">
        
        
           | <a href="http://localhost:1313/blog/1/01/01/single-stepping-the-kernels-c-code/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/1/01/01/single-stepping-the-kernels-c-code/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>Recently, I have had to use the GNU debugger (gdb) connected to a Qemu instance
of a RISC-V processor to step through some kernel code.</p>
<p>Turns out that the Linux kernel is compiled with gcc <code>-O2</code> flag for
optimizations it needs during the build. This causes several problems for a
debugger.  One of them is that the gdb command <code>info registers</code> will show
values as <code>&lt;optimized out&gt;</code>. Another issue is that single-stepping will make
the debugger appear to jump back and forth across lines of code as code is
stepped through with <code>next</code> or <code>step</code> gdb commands.</p>
<p>To circumvent this issue, I ended up with a hack that works well. I don&rsquo;t claim
this is recommended or correct, but it makes it through the build and gdb works
fine. In my debugging, I have wanted to single-step through scheduler code in
the <code>__schedule</code> kernel function. For this purpose, all I have to do is add the
following to <code>kernel/sched/Makefile</code>.</p>
<pre tabindex="0"><code>CFLAGS_REMOVE_core.o := -O2
CFLAGS_core.o := -O0
</code></pre><p>This works brilliantly! What do you think? Let me know in the comments.</p>
<p>Some more tips:</p>
<ul>
<li><code>CONFIG_DEBUG_INFO</code> is needed to ensure kernel has debug symbols for gdb to
load, and ofcourse <code>CONFIG_DEBUG_KERNEL</code>.</li>
<li><code>CONFIG_FRAME_POINTER</code> should be enabled to ensure stack unwinding,
backtraces work correctly.</li>
<li><code>CONFIG_GDB_SCRIPTS</code> is a bunch of useful gdb scripts automatically load when a
vmlinux is gdb&rsquo;d.</li>
</ul>
</div>

    </article>

    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">SRCU state double scan</h1>
    
    
      <p class="meta">
        
        
           | <a href="http://localhost:1313/blog/1/01/01/srcu-state-double-scan/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/1/01/01/srcu-state-double-scan/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>The SRCU flavor of RCU uses per-cpu counters to detect that every CPU has
passed through a quiescent state for a particular SRCU lock instance
(<code>srcu_struct</code>).</p>
<p>There&rsquo;s are total of 4 counters per-cpu. One pair for locks, and another for
unlocks. You can think of the SRCU instance to be split into 2 parts. The
readers sample <code>srcu_idx</code> and decided which part to use. Each part corresponds
to one pair of lock and unlock counters. A reader increments a part&rsquo;s lock
counter during locking and likewise for unlock.</p>
<p>During an update, the updater flips <code>srcu_idx</code> (thus attempting to force new
readers to use the other part) and waits for the lock/unlock counters on the
previous value of <code>srcu_idx</code> to match.  Once the sum of the lock counters of
all CPUs match that of unlock, the system knows all pre-existing read-side
critical sections have completed.</p>
<p>Things are not that simple, however. It is possible that a reader samples the
<code>srcu_idx</code>, but before it can increment the lock counter corresponding to it,
it undergoes a long delay. We thus we end up in a situation where there are
readers in both <code>srcu_idx = 0</code> and <code>srcu_idx = 1</code>.</p>
<p>To prevent such a situation, a writer has to wait for readers corresponding to
both <code>srcu_idx = 0</code> and <code>srcu_idx = 1</code> to complete. This depicted with &lsquo;A MUST&rsquo;
in the below pseudo-code:</p>
<pre tabindex="0"><code>        reader 1        writer                        reader 2
        -------------------------------------------------------
        // read_lock
        // enter
        Read: idx = 0;
        &lt;long delay&gt;    // write_lock
                        // enter
                        wait_for lock[1]==unlock[1]
                        idx = 1; /* flip */
                        wait_for lock[0]==unlock[0]
                        done.
                                                      Read: idx = 1;
        lock[0]++;
                                                      lock[1]++;
                        // write_lock
                        // return
        // read_lock
        // return
        /**** NOW BOTH lock[0] and lock[1] are non-zero!! ****/
                        // write_lock
                        // enter
                        wait_for lock[0]==unlock[0] &lt;- A MUST!
                        idx = 0; /* flip */
                        wait_for lock[1]==unlock[1] &lt;- A MUST!
</code></pre><p>NOTE: QRCU has a similar issue. However it overcomes such a race in the reader
by retrying the sampling of its <code>srcu_idx</code> equivalent.</p>
<p>Q: If you have to wait for readers of both <code>srcu_idx = 0</code>, and <code>1</code>, then why
not just have a single counter and do away with the &ldquo;flipping&rdquo; logic?</p>
<p>Ans:
Because of updater forward progress. If we had a single counter, then it is
possible that new readers would constantly increment the lock counter, thus
updaters would be waiting all the time. By using the &lsquo;flip&rsquo; logic, we are able
to drain pre-existing readers using the inactive part of <code>srcu_idx</code> to be
drained in a bounded time. The number of readers of a &lsquo;flipped&rsquo; part would only
monotonically decrease since new readers go to its counterpart.</p>
<p>2023 update:
I have more detailed notes with diagrams and such on this and other cases. Just
reach out to me if you want to take a look at those.</p>
</div>

    </article>



<div class="pagination">
  
    <a href="/page/2/">&laquo; Previous Post</a> &nbsp; &nbsp; &nbsp;
  

  
    <a href="/page/4/">Next Post&raquo;</a>
  
</div>



</div>


<aside class="sidebar">
  <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="http://localhost:1313/blog/2023/06/25/svm-and-vectors-for-the-curious/">SVM and vectors for the curious</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2018/12/22/dumping-user-and-kernel-stacks-on-kernel-events/">Dumping User and Kernel stacks on Kernel events</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2018/02/10/usdt-for-reliable-userspace-event-tracing/">USDT for reliable Userspace event tracing</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2016/12/31/armv8-flamegraph-and-nmi-support/">ARMv8: flamegraph and NMI support</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2016/06/18/ftrace-events-mechanism/">Ftrace events mechanism</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2016/03/20/tif_need_resched-why-is-it-needed/">TIF_NEED_RESCHED: why is it needed</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2015/12/25/tying-2-voltage-sources/signals-together/">Tying 2 voltage sources/signals together</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2014/06/04/microsd-card-remote-switch/">MicroSD card remote switch</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2014/05/07/linux-spinlock-internals/">Linux Spinlock Internals</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2014/04/24/studying-cache-line-sharing-effects-on-smp-systems/">Studying cache-line sharing effects on SMP systems</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2014/04/22/design-of-fork-followed-by-exec-in-linux/">Design of fork followed by exec in Linux</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/"></a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/bpfd-running-bcc-tools-remotely-across-systems/">BPFd- Running BCC tools remotely across systems</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/c-rvalue-references/">C&#43;&#43; rvalue references</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/categories/">Categories</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/joel/">false</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/figuring-out-herd7-memory-models/">Figuring out herd7 memory models</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/getting-youcompleteme-working-for-kernel-development/">Getting YouCompleteMe working for kernel development</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/gus-global-unbounded-sequences/">GUS (Global Unbounded Sequences)</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/archives/">List of articles</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/making-sense-of-scheduler-deadlocks-in-rcu/">Making sense of scheduler deadlocks in RCU</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/modeling-lack-of-store-ordering-using-pluscal-and-a-wishlist/">Modeling (lack of) store ordering using PlusCal - and a wishlist</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/on-workings-of-hrtimers-slack-time-functionality/">On workings of hrtimer&#39;s slack time functionality</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/powerpc-stack-guard-false-positives-in-linux-kernel/">PowerPC stack guard false positives in Linux kernel</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/rcu-and-dynticks-idle-mode/">RCU and dynticks-idle mode</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/rcu-preempt-what-happens-on-a-context-switch/">RCU-preempt: What happens on a context switch</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/resources/">Resources</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/selinux-debugging-on-chromeos/">SELinux Debugging on ChromeOS</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/single-stepping-the-kernels-c-code/">Single-stepping the kernel&#39;s C code</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/srcu-state-double-scan/">SRCU state double scan</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/understanding-hazard-pointers/">Understanding Hazard Pointers</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>About Me</h1>
  <p>A little something about me.</p>
</section>
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2025 - Joel Fernandes -
  <span class="credit">Powered by <a href="https://gohugo.io">Hugo</a></span>
</p></footer>
  
</body>
</html>