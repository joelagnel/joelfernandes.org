<!DOCTYPE html>


<html class="no-js" lang="en">
<head>
	<meta name="generator" content="Hugo 0.123.7"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  <title>JoelFernandes.org - JoelFernandes.org</title>
  <meta name="author" content="Joel Fernandes">

  
  <meta name="description" content="">

  
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="canonical" href="http://localhost:1313/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="\/javascripts\/libs\/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  
  
</head>
<body   >
  <header role="banner"><div>
<div style="margin-right:50px;float:left;">
  <h1><a href="/">JoelFernandes.org</a></h1>
  
</div>
<div style="float:left;" class="hnav">
 <br>
 
 <a href="/categories/">Blog posts by category.</a><br>
 <a href="/blog/archives/">Archive of all blog posts.</a><br>
 <a href="/resources/">Presentations and other work.</a><br>
</div>
<div style="float:right;">
<img src="/images/peng.png" height=100 width=100>
</div>
</div></header>

  <div id="main">
    <div id="content">
      
<div class="blog-index">

<article>
Hello! I'm Joel and this my personal website built with Hugo! I currently
work at Google. My interests are scheduler, RCU, tracing, synchronization,
memory models and other kernel internals. I also love contributing to the
upstream Linux kernel and other open source projects.

Connect with me on [Twitter](https://twitter.com/joel_linux), and
[LinkedIn](https://www.linkedin.com/in/joelagnel). Or, drop me an email at:
joel _at_ joelfernandes _dot_ org

Look for my name in the kernel git log to find my upstream kernel patches.
Check out [my resume](/joel/joel-resume.pdf) for full details of my work
experience. I also actively present at conferences, see a list of my past
[talks, presentations and publications](/resources).

Full list of all blog posts on this site:

<font face="monospace">
 <li>
   <span>25 Jun 2023</span> &nbsp; <a href="http://localhost:1313/blog/2023/06/25/svm-and-vectors-for-the-curious/">SVM and vectors for the curious</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>22 Dec 2018</span> &nbsp; <a href="http://localhost:1313/blog/2018/12/22/dumping-user-and-kernel-stacks-on-kernel-events/">Dumping User and Kernel stacks on Kernel events</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>10 Feb 2018</span> &nbsp; <a href="http://localhost:1313/blog/2018/02/10/usdt-for-reliable-userspace-event-tracing/">USDT for reliable Userspace event tracing</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>31 Dec 2016</span> &nbsp; <a href="http://localhost:1313/blog/2016/12/31/armv8-flamegraph-and-nmi-support/">ARMv8: flamegraph and NMI support</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>18 Jun 2016</span> &nbsp; <a href="http://localhost:1313/blog/2016/06/18/ftrace-events-mechanism/">Ftrace events mechanism</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>20 Mar 2016</span> &nbsp; <a href="http://localhost:1313/blog/2016/03/20/tif_need_resched-why-is-it-needed/">TIF_NEED_RESCHED: why is it needed</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>25 Dec 2015</span> &nbsp; <a href="http://localhost:1313/blog/2015/12/25/tying-2-voltage-sources/signals-together/">Tying 2 voltage sources/signals together</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>04 Jun 2014</span> &nbsp; <a href="http://localhost:1313/blog/2014/06/04/microsd-card-remote-switch/">MicroSD card remote switch</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>07 May 2014</span> &nbsp; <a href="http://localhost:1313/blog/2014/05/07/linux-spinlock-internals/">Linux Spinlock Internals</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>24 Apr 2014</span> &nbsp; <a href="http://localhost:1313/blog/2014/04/24/studying-cache-line-sharing-effects-on-smp-systems/">Studying cache-line sharing effects on SMP systems</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>22 Apr 2014</span> &nbsp; <a href="http://localhost:1313/blog/2014/04/22/design-of-fork-followed-by-exec-in-linux/">Design of fork followed by exec in Linux</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/"></a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/bpfd-running-bcc-tools-remotely-across-systems/">BPFd- Running BCC tools remotely across systems</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/c-rvalue-references/">C&#43;&#43; rvalue references</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/categories/">Categories</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/joel/">false</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/figuring-out-herd7-memory-models/">Figuring out herd7 memory models</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/getting-youcompleteme-working-for-kernel-development/">Getting YouCompleteMe working for kernel development</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/gus-global-unbounded-sequences/">GUS (Global Unbounded Sequences)</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/archives/">List of articles</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/making-sense-of-scheduler-deadlocks-in-rcu/">Making sense of scheduler deadlocks in RCU</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/modeling-lack-of-store-ordering-using-pluscal-and-a-wishlist/">Modeling (lack of) store ordering using PlusCal - and a wishlist</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/on-workings-of-hrtimers-slack-time-functionality/">On workings of hrtimer&#39;s slack time functionality</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/powerpc-stack-guard-false-positives-in-linux-kernel/">PowerPC stack guard false positives in Linux kernel</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/rcu-and-dynticks-idle-mode/">RCU and dynticks-idle mode</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/rcu-preempt-what-happens-on-a-context-switch/">RCU-preempt: What happens on a context switch</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/resources/">Resources</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/selinux-debugging-on-chromeos/">SELinux Debugging on ChromeOS</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/single-stepping-the-kernels-c-code/">Single-stepping the kernel&#39;s C code</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/srcu-state-double-scan/">SRCU state double scan</a> 
 </li>
</font>

<font face="monospace">
 <li>
   <span>01 Jan 0001</span> &nbsp; <a href="http://localhost:1313/blog/1/01/01/understanding-hazard-pointers/">Understanding Hazard Pointers</a> 
 </li>
</font>

</article>


 


    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">SVM and vectors for the curious</h1>
    
    
      <p class="meta">
        
<time datetime="2023-06-25T00:00:00Z" pubdate data-updated="true">Jun 25, 2023</time>

        
           | <a href="http://localhost:1313/blog/2023/06/25/svm-and-vectors-for-the-curious/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/2023/06/25/svm-and-vectors-for-the-curious/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>I posted a pair of articles/notes on SVM and vectors. I was curious how they
worked and I&rsquo;m sure these notes will be useful to me or others. Especially
for some basic mathematics related to machine learning.</p>
<p><a href="/resources/ai_vectors.pdf">Notes on Vectors</a></p>
<p><a href="/resources/ai_svm.pdf">Notes on SVM</a></p>
</div>

    </article>

    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">Dumping User and Kernel stacks on Kernel events</h1>
    
    
      <p class="meta">
        
<time datetime="2018-12-22T20:28:24-05:00" pubdate data-updated="true">Dec 22, 2018</time>

        
           | <a href="http://localhost:1313/blog/2018/12/22/dumping-user-and-kernel-stacks-on-kernel-events/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/2018/12/22/dumping-user-and-kernel-stacks-on-kernel-events/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>Dumping the native kernel and userspace stack when a certain path in the kernel
or userspace occurs, can be useful to understand which code paths triggered a
certain behavior that you&rsquo;re trying to debug, such as an error you found in the
log. One such case is when you notice Selinux denial messages in logs but want
to know which path triggered it.</p>
<p>In this article we will show you how to use kernel instrumentation and BCC to
dump the both the user and kernel stack. The article applies both to Android
and regular Linux kernels.</p>
<h2 id="example-understanding-which-path-triggered-an-selinux-denial">Example: Understanding which path triggered an SELinux denial</h2>
<h3 id="step-1-add-a-tracepoint-to-the-kernel">Step 1: Add a tracepoint to the kernel</h3>
<p>Apply the following diff to your kernel. It adds a tracepoint at precisely the
point where an SELinux denial is logged. If not cleanly applying, patch it in
manually.</p>
<pre tabindex="0"><code>Diff to add a tracepoint for selinux denials

diff --git a/include/trace/events/selinux.h b/include/trace/events/selinux.h
new file mode 100644
index 000000000000..dac185062634
--- /dev/null
+++ b/include/trace/events/selinux.h
@@ -0,0 +1,34 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM selinux
+
+#if !defined(_TRACE_SELINUX_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_SELINUX_H
+
+#include &lt;linux/ktime.h&gt;
+#include &lt;linux/tracepoint.h&gt;
+
+TRACE_EVENT(selinux_denied,
+
+	TP_PROTO(int cls, int av),
+
+	TP_ARGS(cls, av),
+
+	TP_STRUCT__entry(
+		__field(	int,		cls	)
+		__field(	int,		av	)
+	),
+
+	TP_fast_assign(
+		__entry-&gt;cls = cls;
+		__entry-&gt;av = av;
+	),
+
+	TP_printk(&#34;denied %d %d&#34;,
+		__entry-&gt;cls,
+		__entry-&gt;av)
+);
+
+#endif /* _TRACE_SELINUX_H */
+
+/* This part ust be outside protection */
+#include &lt;trace/define_trace.h&gt;
diff --git a/security/selinux/avc.c b/security/selinux/avc.c
index 84d9a2e2bbaf..ab04b7c2dd01 100644
--- a/security/selinux/avc.c
+++ b/security/selinux/avc.c
@@ -34,6 +34,9 @@
 #include &#34;avc_ss.h&#34;
 #include &#34;classmap.h&#34;
 
+#define CREATE_TRACE_POINTS
+#include &lt;trace/events/selinux.h&gt;
+
 #define AVC_CACHE_SLOTS			512
 #define AVC_DEF_CACHE_THRESHOLD		512
 #define AVC_CACHE_RECLAIM		16
@@ -713,6 +716,12 @@ static void avc_audit_pre_callback(struct audit_buffer *ab, void *a)
 	struct common_audit_data *ad = a;
 	audit_log_format(ab, &#34;avc:  %s &#34;,
 			 ad-&gt;selinux_audit_data-&gt;denied ? &#34;denied&#34; : &#34;granted&#34;);
+
+	if (ad-&gt;selinux_audit_data-&gt;denied) {
+		trace_selinux_denied(ad-&gt;selinux_audit_data-&gt;tclass,
+				     ad-&gt;selinux_audit_data-&gt;audited);
+	}
+
 	avc_dump_av(ab, ad-&gt;selinux_audit_data-&gt;tclass,
 			ad-&gt;selinux_audit_data-&gt;audited);
 	audit_log_format(ab, &#34; for &#34;);
</code></pre><h3 id="step-2-install-adebhttpsgithubcomjoelagneladeb">Step 2: Install <a href="https://github.com/joelagnel/adeb">adeb</a></h3>
<p>Run the command:</p>
<pre tabindex="0"><code>adeb prepare --full
</code></pre><p>This also installs BCC on the Android device which contains the &rsquo;trace&rsquo; utility
we need for the next step. For regular Linux kernels, you may have to <a href="https://github.com/joelagnel/bcc/blob/master/README.md">manually
install BCC</a> or find a
package for it.</p>
<h3 id="step-3-start-tracing-the-user-and-kernel-stacks">Step 3: Start tracing the user and kernel stacks</h3>
<p>Running the following command:</p>
<pre tabindex="0"><code>adeb shell
trace -K -U &#39;t:selinux:selinux_denial&#39;
</code></pre><p>You should see something like this when denials are triggered:</p>
<pre tabindex="0"><code>2286    2434    Binder:2286_4   selinux_denied   

        avc_audit_pre_callback+0xd8 [kernel]
        avc_audit_pre_callback+0xd8 [kernel]
        common_lsm_audit+0x64 [kernel]
        slow_avc_audit+0x74 [kernel]
        avc_has_perm+0xb8 [kernel]
        selinux_binder_transfer_file+0x158 [kernel]
        security_binder_transfer_file+0x50 [kernel]
        binder_translate_fd+0xcc [kernel]
        binder_transaction+0x1b64 [kernel]
        binder_ioctl+0xadc [kernel]
        do_vfs_ioctl+0x5c8 [kernel]
        sys_ioctl+0x88 [kernel]
        __sys_trace_return+0x0 [kernel]
        __ioctl+0x8 [libc.so]
        android::IPCThreadState::talkWithDriver(bool)+0x104 [libbinder.so]
        android::IPCThreadState::waitForResponse(android::Parcel*, int*)+0x40
                                                            [libbinder.so]
        android::IPCThreadState::executeCommand(int)+0x460 [libbinder.so]
        android::IPCThreadState::getAndExecuteCommand()+0xa0 [libbinder.so]
        android::IPCThreadState::joinThreadPool(bool)+0x40 [libbinder.so]
        [unknown] [libbinder.so]
        android::Thread::_threadLoop(void*)+0x12c [libutils.so]
        android::AndroidRuntime::javaThreadShell(void*)+0x90 [libandroid_runtime.so]
        __pthread_start(void*)+0x28 [libc.so]
        __start_thread+0x48 [libc.so]
</code></pre><p>The same trick can be used for dumping the stack on syscalls, random kernel
functions using kprobes and more! Just change the arguments passed to the
&rsquo;trace&rsquo; command.</p>
</div>

    </article>

    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">USDT for reliable Userspace event tracing</h1>
    
    
      <p class="meta">
        
<time datetime="2018-02-10T23:22:39-08:00" pubdate data-updated="true">Feb 10, 2018</time>

        
           | <a href="http://localhost:1313/blog/2018/02/10/usdt-for-reliable-userspace-event-tracing/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/2018/02/10/usdt-for-reliable-userspace-event-tracing/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>Userspace program when compiled to native can place tracepoints in them using a USDT (User Statically Defined Tracing), the details of the tracepoints (such as address and arguments) are placed as a note in the ELF binary for other tools to interpret. This at first seems much better than using Uprobes directly on userspace functions, since the latter not only needs symbol information from the binary but is also at the mercy of compiler optimizations and function inlining. In Android we directly write into <code>tracing_mark_write</code> for userspace tracepoints. While this has its advantages (simplicity), it also means that the only way to &ldquo;process&rdquo; the trace information for ALL tracing usecases is by reading back the trace buffer to userspace and processing them offline, thus needing a full user-kernel-user round trip. Further its not possible to easily create triggers on these events (dump the stack or stop tracing when an event fires, for example). Uprobes event tracing on the other hands gets the full benefit that ftrace events do. Also all userspace emitted trace data is a string which has to be parsed and post-processed. USDT seems a nice way to solve some of these problems, since we can then use the user-provided data and create Uprobes at the correct locations, and process these on the fly using BCC without things ever hitting the trace buffer or returning to userspace.</p>
<p>This simple article is based on some notes I made while playing with USDT probes. To build a C program with an SDT probe, you can just include <code>sdt.h</code> and <code>sdt-config.h</code> from the Ubuntu systemtap-sdt-devel package, which works for both arm64 and x86.</p>
<p>C program can be as simple as:</p>
<pre tabindex="0"><code>#include &#34;sdt.h&#34;
int main() {
	DTRACE_PROBE(&#34;test&#34;, &#34;probe1&#34;);
}
</code></pre><p>One compiling this, a new <code>.note.stapsdt</code> ELF section is created, which can be read by:
<code>readelf -n &lt;bin-path&gt;</code></p>
<pre tabindex="0"><code>Displaying notes found in: .note.stapsdt
  Owner                 Data size	Description
  stapsdt              0x00000029	NT_STAPSDT (SystemTap probe descriptors)
    Provider: &#34;test&#34;
    Name: &#34;probe1&#34;
    Location: 0x0000000000000664, Base: 0x00000000000006f4, Semaphore: 0x0000000000000000
    Arguments: 
</code></pre><p>Here there are no arguments, however we can use <code>DTRACE_PROBE2</code> to pass more, for example for 2 of them:</p>
<pre tabindex="0"><code>int main() {
	int a = 1;
	int b = 2;

	DTRACE_PROBE2(&#34;test&#34;, &#34;probe1&#34;, a, b);
}
</code></pre><p>The <code>readelf</code> tool now reads:</p>
<pre tabindex="0"><code>Displaying notes found in: .note.stapsdt
  Owner                 Data size	Description
  stapsdt              0x00000040	NT_STAPSDT (SystemTap probe descriptors)
    Provider: &#34;test&#34;
    Name: &#34;probe1&#34;
    Location: 0x0000000000000672, Base: 0x0000000000000704, Semaphore: 0x0000000000000000
    Arguments: -4@-4(%rbp) -4@-8(%rbp)
</code></pre><p>Notice how the arguments show exactly how to access the parameters at the location. In this case, we know the arguments are on the stack and at momention offsets from the base pointer.</p>
<p>Compiling with <code>-f-omit-frame-pointer</code> shows the following in <code>readelf</code>:</p>
<pre tabindex="0"><code>Displaying notes found in: .note.stapsdt
  Owner                 Data size	Description
  stapsdt              0x00000040	NT_STAPSDT (SystemTap probe descriptors)
    Provider: &#34;test&#34;
    Name: &#34;probe1&#34;
    Location: 0x0000000000000670, Base: 0x0000000000000704, Semaphore: 0x0000000000000000
    Arguments: -4@-4(%rsp) -4@-8(%rsp)
</code></pre><p>Without the base pointer, the compiler relies on the stack pointer for the arguments. However notice that even though the C program is identical to the previous example, the &ldquo;arguments&rdquo; in the note section of the ELF has changed. This dynamic nature is one of the key reasons why SDT probes are so much better than say using <code>perf probe</code> directly to install Uprobes in the wild. With some help userspace and the compiler, we have more reliable information to access arguments without needing any DWARF debug info.</p>
<p>Compiling with ARM64 gcc shows a slightly different output for the Arguments. Note that that Arguments is just a string which can be post processed by tools to fetch the probe data.</p>
<pre tabindex="0"><code>Displaying notes found in: .note.stapsdt
  Owner                 Data size	Description
  stapsdt              0x0000003f	NT_STAPSDT (SystemTap probe descriptors)
    Provider: &#34;test&#34;
    Name: &#34;probe1&#34;
    Location: 0x000000000000077c, Base: 0x0000000000000820, Semaphore: 0x0000000000000000
    Arguments: -4@[sp, 12] -4@[sp, 8]
</code></pre><p>USDT limitations as I see it:</p>
<ul>
<li>
<p>No information about types is stored. This is kind of sad, since now inorder to know what do with the values, one needs more information. These tracepoints were used with DTrace and SystemTap and turns out the scripts that probe these tracepoints are where the type information is stored or assumed. Uprobes tracer supports &ldquo;string&rdquo; but without knowing that the USDT is a string value, there&rsquo;s no way a Uprobe can be created on it, since all the stap note tells us is that there&rsquo;s a pointer there (who knows if its a 64-bit integer or a character pointer, for example).</p>
</li>
<li>
<p>Argument names are also not stored. This means arguments have to be in the same order in the debug script as they are in the program being debugged.</p>
</li>
</ul>
<p>It seems with a little bit of work, both these things can be added. Does that warrant a new section or can the stapsdt section be augment without causing breakage of existing tools? I don&rsquo;t know yet.</p>
<h2 id="sdt-parsing-logic">SDT parsing logic</h2>
<p>BCC has a USDT parser written to extract probes from the ELF. Read <code>parse_stapsdt_note</code> in <code>src/cc/bcc_elf.c</code> in the BCC tree for details.</p>
<h2 id="dynamic-programming-languages">Dynamic programming languages</h2>
<p>Programs that are interpretted can&rsquo;t provide this information ahead of time. The <a href="https://github.com/sthima/libstapsdt#how-it-works">libstapsdt</a> tries to solve this by <a href="https://github.com/sthima/libstapsdt/blob/6045277309ff0425322bed5e71393ce5c8fa1344/src/libstapsdt.c#L89">creating a shared library on the fly</a> and linking to it from the dynamic program. This seems a bit fragile but appears to have users. There are wrappers in Python and Nodejs. <a href="https://medium.com/sthima-insights/we-just-got-a-new-super-power-runtime-usdt-comes-to-linux-814dc47e909f">Check this article</a> for more details.</p>
<p>Open question I have:</p>
<ul>
<li>Do any existing Linux tools handle USDT strings? Uprobe tracer does support strings, so the infrastructure seems to be there. I didn&rsquo;t see any hints of this in <a href="src/cc/usdt/usdt_args.cc">BCC</a>. Neither does <a href="https://github.com/sthima/libstapsdt/blob/6045277309ff0425322bed5e71393ce5c8fa1344/src/libstapsdt.h#L14">libstapsdt</a> seem to have this.</li>
</ul>
<h2 id="other-ideas">Other ideas</h2>
<ul>
<li>
<p>Creating Uprobes on the fly when a process is loaded: Ideally speaking, if the ELF note section had all the information that the kernel needed, then we could create the Uprobe events for the uprobe trace events at load time and keep them disabled without needing userspace to do anything else. This seems crude at first, but in the &ldquo;default&rdquo; case, it would still have almost no-overhead. This does mean that all the information Uprobe tracing needs will have to be stored in the note section. The other nice thing about this is, you no longer need to know the PID of all processes with USDTs in them. EDIT: This idea is flawed. Uprobes are created before a process is loaded AFAIU now, using the binary path of the executable and libraries. What&rsquo;s more useful is to maintain a cache of all executables in the file system, and their respective instrumentation points. Then on boot up, perhaps we can create all necessary uprobes from early userspace.</p>
</li>
<li>
<p>For dynamic languages, <code>libstapsdt</code> seems great, but it feels a bit hackish since it creates a temporary file for the stub. Perhaps uprobes can be created after the temporary file is dlopen&rsquo;ed and then the file can be unlinked if it hasn&rsquo;t been already, so that there aren&rsquo;t any more references to the temporary file in the file system. Such a temporary file could also probably be in a RAM based file system perhaps.</p>
</li>
</ul>
<h2 id="references">References</h2>
<ol>
<li><a href="http://www.brendangregg.com/blog/2015-07-03/hacking-linux-usdt-ftrace.html">Brendan Gregg&rsquo;s USDT ftrace page</a></li>
<li><a href="https://www.kernel.org/doc/Documentation/trace/uprobetracer.txt">Uprobe tracer in the kernel</a></li>
<li><a href="https://medium.com/sthima-insights/we-just-got-a-new-super-power-runtime-usdt-comes-to-linux-814dc47e909f">USDT for dynamic languages</a></li>
<li><a href="https://dzone.com/articles/next-generation-linux-tracing">Sasha Goldstein&rsquo;s &ldquo;Next Generation Linux Tracing With BPF&rdquo; article</a></li>
<li><a href="https://sourceware.org/systemtap/wiki/UserSpaceProbeImplementation">SystemTap SDT implementation</a></li>
</ol>
</div>

    </article>

    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">ARMv8: flamegraph and NMI support</h1>
    
    
      <p class="meta">
        
<time datetime="2016-12-31T22:29:26-07:00" pubdate data-updated="true">Dec 31, 2016</time>

        
           | <a href="http://localhost:1313/blog/2016/12/31/armv8-flamegraph-and-nmi-support/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/2016/12/31/armv8-flamegraph-and-nmi-support/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>Non-maskable interrupts (NMI) is a really useful feature for debugging, that hardware can provide. Unfortunately ARM doesn&rsquo;t provide an out-of-the-box NMI interrupt mechanism. This post shows a flamegraph issue due to missing NMI support, and the upstream work being done to simulate NMI in ARMv8.</p>
<p>Some great Linux kernel features that rely on NMI to work properly are:</p>
<ul>
<li>
<p>Backtrace from all CPUs: A number of places in the kernel rely on dumping the stacks of all CPUs at the time of a failure to determine what was going on. Some of them are <a href="http://lxr.free-electrons.com/source/kernel/hung_task.c">Hung Task detection</a>, <a href="http://lxr.free-electrons.com/source/Documentation/lockup-watchdogs.txt">Hard/soft lockup detector</a> and spinlock debugging code.</p>
</li>
<li>
<p>Perf profiling and flamegraphs: To be able to profile code that runs in interrupt handlers, or in sections of code that disable interrupts, Perf relies on NMI support in the architecture. <a href="http://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html">flamegraphs</a> are a great visual representation of perf profile output. Below is a flamegraph I generated from perf profile output, that shows just what happens on an architecture like ARMv8 with missing NMI support. Perf is using maskable-interrupts on this platform for profiling:</p>
</li>
</ul>
<p>{% img /images/nmi/flamegraph.png %}</p>
<p>As you can see in the area of the flamegraph where the arrow is pointed, a large amount of time is spent in <code>_raw_spin_unlock_irqrestore</code>. It can baffle anyone looking at this data for the first time, and make them think that most of the time is spent in the unlock function. What&rsquo;s actually happenning is because perf is using a maskable interrupt in ARMv8 to do its profiling, any section of code that disables interrupts will not be see in the flamegraph (not be profiled). In other words perf is unable to peek into sections of code where interrupts are disabled. As a result, when interrupts are reenabled during the <code>_raw_spin_unlock_irqrestore</code>, the perf interrupt routine then kicks in and records the large number of samples that elapsed in the interrupt-disable section but falsely accounts it to the _raw_spin_unlock_restore function during which the perf interrupt got a chance to run. Hence the flamegraph anomaly. It is indeed quite sad that ARM still doesn&rsquo;t have a true NMI which perf would love to make use of.</p>
<p>BUT! <a href="https://lkml.org/lkml/2016/8/19/583">Daniel Thompson</a> has been hard at work trying to simulate Non-maskable interrupts on ARMv8. The idea is <a href="/misc/arm-irq-priortization-white-paper.pdf">based on using interrupt priorities</a> and is the subject of the rest of this post.</p>
<h2 id="nmi-simulation-using-priorities">NMI Simulation using priorities</h2>
<p>To simulate an NMI, Daniel creates 2 groups of interrupts in his patchset. One group is for all &rsquo;normal&rsquo; interrupts, and the other for non-maskable interrupts (NMI). Non-maskable interrupts are assigned a higher priority than the normal interrupt group. Inorder to &lsquo;mask&rsquo; interrupts in this approach, Daniel replaces the regular interrupt masking scheme in the kernel which happens at the CPU-core level, with setting of the interrupt controller&rsquo;s PMR (priority mask register). When the PMR is set to a certain value, only interrupts which have a higher priority than what&rsquo;s in the PMR will be signaled to a CPU core, all other interrupts will be silenced (masked). By using this technique, it is possible to mask normal interrupts while keeping the NMI unmasked all the time.</p>
<p>Just how does he do this? So, a small primer on interrupts in the ARM world.
ARM uses the GIC <a href="http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dai0176c/ar01s03s01.html">Generic interrupt controller</a> to prioritize and route interrupts to CPU cores. GIC interrupt priorties go from 0 to 255. 0 being highest and 255 being the lowest. By default, the kernel <a href="http://lxr.free-electrons.com/source/include/linux/irqchip/arm-gic.h?v=4.8#L57">assigns priority 0xa0 (192)</a> to all interrupts. He changes this default priority from 0xa0 to 0xc0 (you&rsquo;ll see why).
He then defines what values of PMR would be consider as &ldquo;unmasked&rdquo; vs &ldquo;masked&rdquo;. Masked is 0xb0 and unmasked is 0xf0. This results in the following priorities (greater numbers are lower priority).</p>
<pre tabindex="0"><code>0xf0 (240 decimal)  (11110000 binary) - Interrupts Unmasked (enabled)
0xc0 (192 decimal)  (11000000 binary) - Normal interrupt priority
0xb0 (176 decimal)  (10110000 binary) - Interrupts masked   (disabled)
0x80 (128 decimal)  (10000000 binary) - Non-maskable interrupts
</code></pre><p>In this new scheme, when interrupts are to be masked (disabled), the PMR is set to 0xf0 and when they are unmasked (enabled), the PMR is set to 0xb0. As you can see, setting the PMR to 0xb0 indeed masks normal interrupts, because 0xb0(PMR) &lt; 0xc0(Normal), however non-maskable interrupts still stay unmasked as 0x80(NMI) &lt; 0xb0(PMR). Also notice that inorder to mask/unmask interrupts, all that needs to be done is flip bit 7 in the PMR (0xb0 -&gt; 0xf0). Daniel largely uses Bit 7 as the mask bit in the patchset.</p>
<h2 id="quirk-1-saving-of-the-pmr-context-during-traps">Quirk 1: Saving of the PMR context during traps</h2>
<p>Its suggested in the patchset that during traps, the priority value set in the PMR needs to be saved because it may change during traps. To facilitate this, Daniel found a dummy bit in the PSTATE register (PSR). During any exception, Bit 7 of of the PMR is saved into a PSR bit (he calls it the G bit) and restores it on return from the exception. Look at the changes to <code>kernel_entry</code> macro in the set for this code.</p>
<h2 id="quirk-2-ack-of-masked-interrupts">Quirk 2: Ack of masked interrupts</h2>
<p>Note that interrupts are masked before the GIC interrupt controller code can even identify the source of the interrupt. When the GIC code eventually runs, it is tasked with identifying the interrupt source. It does so by reading the <code>IAR</code> register. This read also has the affecting of &ldquo;Acking&rdquo; the interrupt - in other words, telling the GIC that the kernel has acknowledged the interrupt request for that particular source. Daniel points out that, because the new scheme uses PMR for interrupt masking, its no longer possible to ACK interrupts without first unmasking them (by resetting the PMR) so he temporarily resets PMR, does the <code>IAR</code> read, and restores it. Look for the code in <code>gic_read_iar_common</code> in his patchset to handle this case.</p>
<h2 id="open-questions-i-have">Open questions I have</h2>
<ul>
<li>Where in the patchset does Daniel mask NMIs once an NMI is in progress, or is this even needed?</li>
</ul>
<h2 id="future-work">Future work</h2>
<p>Daniel has tested his patchset only on the foundation model yet, but it appears that the patch series with modifications should work on the newer Qualcomm chipsets that have the necessary GIC (Generic interrupt controller) access from the core to mess with IRQ priorities. Also, currently Daniel has only implemented CPU backtrace, more work needs to be done for perf support which I&rsquo;ll look into if I can get backtraces working properly on real silicon first.</p>
</div>

    </article>

    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">Ftrace events mechanism</h1>
    
    
      <p class="meta">
        
<time datetime="2016-06-18T22:29:26-07:00" pubdate data-updated="true">Jun 18, 2016</time>

        
           | <a href="http://localhost:1313/blog/2016/06/18/ftrace-events-mechanism/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/2016/06/18/ftrace-events-mechanism/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>Ftrace events are a mechanism that allows different pieces of code in the kernel to &lsquo;broadcast&rsquo; events of interest. Such as a scheduler context-switch <code>sched_switch</code> for example. In the scheduler core&rsquo;s <code>__schedule</code> function, you&rsquo;ll see something like: <code>trace_sched_switch(preempt, prev, next);</code>
This immediately results in a write to a per-cpu ring buffer storing info about what the previous task was, what the next one is, and whether the switch is happening as a result of kernel preemption (versus happening for other reasons such as a task waiting for I/O completion).</p>
<p>Under the hood, these ftrace events are actually implemented using tracepoints. The terms events are tracepoints appear to be used interchangeably, but it appears one could use a trace point if desired without having to do anything with ftrace. Events on the other hand use ftrace.</p>
<p>Let&rsquo;s discuss a bit about how a tracepoint works. Tracepoints are hooks that are inserted into points of code of interest and call a certain function of your choice (also known as a function probe). Inorder for the tracepoint to do anything, you have to register a function using <code>tracepoint_probe_register</code>. Multiple functions can be registered in a single hook. Once your tracepoint is hit, all functions registered to the tracepoint are executed. Also note that if no function is registered to the tracepoint, then the tracepoint is essentially a NOP with zero-overhead. Actually that&rsquo;s a lie, there is a branch (and some space) overhead only although negligible.</p>
<p>Here is the heart of the code that executes when a tracepoint is hit:</p>
<pre tabindex="0"><code>#define __DECLARE_TRACE(name, proto, args, cond, data_proto, data_args) \
        extern struct tracepoint __tracepoint_##name;                   \
        static inline void trace_##name(proto)                          \
        {                                                               \
                if (static_key_false(&amp;__tracepoint_##name.key))         \
                        __DO_TRACE(&amp;__tracepoint_##name,                \
                                TP_PROTO(data_proto),                   \
                                TP_ARGS(data_args),                     \
                                TP_CONDITION(cond),,);                  \
                if (IS_ENABLED(CONFIG_LOCKDEP) &amp;&amp; (cond)) {             \
                        rcu_read_lock_sched_notrace();                  \
                        rcu_dereference_sched(__tracepoint_##name.funcs);\
                        rcu_read_unlock_sched_notrace();                \
                }                                                       \
        }                                                   
</code></pre><p>The <code>static_key_false</code> in the above code will evaluate to false if there&rsquo;s no probe registered to the tracepoint.</p>
<p>Digging further, <code>__DO_TRACE</code> does the following in <code>include/linux/tracepoint.h</code></p>
<pre tabindex="0"><code>#define __DO_TRACE(tp, proto, args, cond, prercu, postrcu)              \
        do {                                                            \
                struct tracepoint_func *it_func_ptr;                    \
                void *it_func;                                          \
                void *__data;                                           \
                                                                        \
                if (!(cond))                                            \
                        return;                                         \
                prercu;                                                 \
                rcu_read_lock_sched_notrace();                          \
                it_func_ptr = rcu_dereference_sched((tp)-&gt;funcs);       \
                if (it_func_ptr) {                                      \
                        do {                                            \
                                it_func = (it_func_ptr)-&gt;func;          \
                                __data = (it_func_ptr)-&gt;data;           \
                                ((void(*)(proto))(it_func))(args);      \
                        } while ((++it_func_ptr)-&gt;func);                \
                }                                                       \
                rcu_read_unlock_sched_notrace();                        \
                postrcu;                                                \
        } while (0)
</code></pre><p>There&rsquo;s a lot going on there, but main part is the loop that goes through all function pointers (probes) that were registered to the tracepoint and calls them one after the other.</p>
<p>Now, here&rsquo;s some secrets. Since all ftrace events are tracepoints under the hood, you can piggy back onto interesting events in your kernel with your own probes. This allows you to write interesting tracers. Infact this is precisely how blktrace works, and also is how SystemTap hooks into ftrace events.
<a href="https://github.com/joelagnel/joel-snips/blob/master/k-patches/cpuhists.diff">Checkout a module I wrote</a> that hooks onto <code>sched_switch</code> to build some histograms. The code there is still buggy but if you mess with it and improve it please share your work.</p>
<p>Now that we know a good amount about tracepoints, ftrace events are easy.</p>
<p>An ftrace event being based on tracepoints, makes full use of it but it has to do more. Ofcourse, it has to write events out to the ring buffer.
When you enable an ftrace event using debug-fs, at that instant the ftrace events framework registers an &ldquo;event probe&rdquo; function at the tracepoint that represents the event. How? Using <code>tracepoint_probe_register</code> just as we discussed.</p>
<p>The code for this is in the file <code>kernel/trace/trace_events.c</code> in function <code>trace_event_reg</code>.</p>
<pre tabindex="0"><code>int trace_event_reg(struct trace_event_call *call,
                    enum trace_reg type, void *data)
{
        struct trace_event_file *file = data;

        WARN_ON(!(call-&gt;flags &amp; TRACE_EVENT_FL_TRACEPOINT));
        switch (type) {
        case TRACE_REG_REGISTER:
                return tracepoint_probe_register(call-&gt;tp,
                                                 call-&gt;class-&gt;probe,
                                                 file);
...
</code></pre><p>The probe function <code>call-&gt;class-&gt;probe</code> for trace events is defined in the file <code>include/trace/trace_events.h</code> and does the job of writing to the ring buffer. In a nutshell, the code gets a handle into the ring buffer, does assignment of the values to the entry structure and writes it out. There is some magic going on here to accomodate arbitrary number of arguments but I am yet to figure that out.</p>
<pre tabindex="0"><code>static notrace void                                                     \
trace_event_raw_event_##call(void *__data, proto)                       \
{                                                                       \
        struct trace_event_file *trace_file = __data;                   \
        struct trace_event_data_offsets_##call __maybe_unused __data_offsets;\
        struct trace_event_buffer fbuffer;                              \
        struct trace_event_raw_##call *entry;                           \
        int __data_size;                                                \
                                                                        \
        if (trace_trigger_soft_disabled(trace_file))                    \
                return;                                                 \
                                                                        \
        __data_size = trace_event_get_offsets_##call(&amp;__data_offsets, args); \
                                                                        \
        entry = trace_event_buffer_reserve(&amp;fbuffer, trace_file,        \
                                 sizeof(*entry) + __data_size);         \
                                                                        \
        if (!entry)                                                     \
                return;                                                 \
                                                                        \
        tstruct                                                         \
                                                                        \
        { assign; }                                                     \
                                                                        \
        trace_event_buffer_commit(&amp;fbuffer);                            \
}
</code></pre><p>Let me know any comments you have or any other ftrace event behavior you&rsquo;d like explained.</p>
</div>

    </article>

    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">TIF_NEED_RESCHED: why is it needed</h1>
    
    
      <p class="meta">
        
<time datetime="2016-03-20T01:44:32-07:00" pubdate data-updated="true">Mar 20, 2016</time>

        
           | <a href="http://localhost:1313/blog/2016/03/20/tif_need_resched-why-is-it-needed/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/2016/03/20/tif_need_resched-why-is-it-needed/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p><code>TIF_NEED_RESCHED</code> is one of the many &ldquo;thread information flags&rdquo; stored along side every task in the Linux Kernel. One of the flags which is vital to the working of preemption is <code>TIF_NEED_RESCHED</code>. Inorder to explain why its important and how it works, I will go over 2 cases where <code>TIF_NEED_RESCHED</code> is used.</p>
<h2 id="preemption">Preemption</h2>
<p>Preemption is the process of forceably grabbing CPU from a user or kernel context and giving it to someone else (user or kernel). It is the means for timesharing a CPU between competing tasks (I will use task as terminology for process).
In Linux, the way it works is a timer interrupt (called the tick) interrupts the task that is running and makes a decision about whether a task or a kernel code path (executing on behalf of a task like in a syscall) is to be preempted. This decision is based on whether the task has been running long-enough and something higher priority woke up and needs CPU now, or is ready to run.</p>
<p>These things happen in <code>scheduler_tick()</code>, the exact path is <em>TIMER HARDWARE INTERRUPT</em> -&gt; <code>scheduler_tick</code> -&gt; <code>task_tick_fair</code> -&gt; <code>entity_tick</code> -&gt; <code>check_preempt_tick</code>.
<code>entity_tick()</code> updates various run time statistics of the task, and <code>check_preempt_tick()</code> is where TIF_NEED_RESCHED is set.</p>
<p>Here&rsquo;s a small bit of code in <code>check_preempt_tick</code></p>
<pre tabindex="0"><code>check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)
{
        unsigned long ideal_runtime, delta_exec;
        struct sched_entity *se;
        s64 delta;

        ideal_runtime = sched_slice(cfs_rq, curr);
        delta_exec = curr-&gt;sum_exec_runtime - curr-&gt;prev_sum_exec_runtime;
        if (delta_exec &gt; ideal_runtime) {
                resched_curr(rq_of(cfs_rq));
                /*
                 * The current task ran long enough, ensure it doesn&#39;t get
                 * re-elected due to buddy favours.
                 */
                clear_buddies(cfs_rq, curr);
                return;
        }
</code></pre><p>Here you see a decision is made that the process ran long enough based on its runtime and if so call <code>resched_curr</code>. Turns out <code>resched_curr</code> sets the <code>TIF_NEED_RESCHED</code> for the current task! This informs whoever looks at the flag, that this process should be scheduled out soon.</p>
<p>Even though this flag is set at this point, the task is not going to be preempted yet. This is because preemption happens at specific points such as exit of interrupts. If the flag is set because the timer interrupt (scheduler decided) decided that something of higher priority needs CPU now and sets <code>TIF_NEED_RESCHED</code>, then at the exit of the timer interrupt (interrupt exit path), <code>TIF_NEED_RESCHED</code> is checked, and because it is set - <code>schedule()</code> is called causing context switch to happen to another process of higher priority, instead of just returning to the existing process that the timer interrupted normally would.
Lets examine where this happens.</p>
<p>For return from interrupt to user-mode:</p>
<p>If the tick interrupt happened user-mode code was running, then in somewhere in the interrupt exit path for x86, this call chain calls schedule <code>ret_from_intr</code> -&gt; <code>reint_user</code> -&gt; <code>prepare_exit_to_usermode</code>. Here the need_reched flag is checked, and if true <code>schedule()</code> is called.</p>
<p>For return from interrupt to kernel mode, things are a bit different (skip this para if you think it&rsquo;ll confuse you).</p>
<p>This feature requires kernel preemption to be enabled. The call chain doing the preemption is: <code>ret_from_intr</code> -&gt; <code>reint_kernel</code> -&gt; <code>preempt_schedule_irq</code> (see <code>arch/x86/entry/entry_64.S</code>) which calls <code>schedule</code>.
Note that, for return to kernel mode, I see that <code>preempt_schedule_irq</code> calls <code>schedule</code> anyway whether need_resched flag is set or not, this is probably Ok but I am wondering if need_resched should be checked here before <code>schedule</code> is called. Perhaps it would be an optimiziation to avoid unecessarily calling <code>schedule</code>. One reason for not doing so would be, say any other interrupt other than timer tick is returning to the interrupted kernel space, then in these cases for example - if the timer tick didn&rsquo;t get a chance to run (because all other local interrupts are disabled in Linux until an interrupt finishes, in this case our non-timer interrupt), then we&rsquo;d want the exit path of the non-timer interrupt to behave just like the exit path of the timer tick interrupt would behave, whether need_resched is set or not.</p>
<h2 id="critical-sections-in-kernel-code-where-preemption-is-off">Critical sections in kernel code where preemption is off</h2>
<p>One nice example of a code path where preemption is off is the <code>mutex_lock</code> path in the kernel. In the kernel, there is an optimization where if a mutex is already locked and not available, but if the lock owner (the task currently holding the lock) is running on another CPU, then the mutex temporarily becomes a spinlock (which means it will spin until it can get the lock) instead of behaving like a mutex (which sleeps until the lock is available). The pseudo code looks like this:</p>
<pre tabindex="0"><code>mutex_lock() {
  disable_preempt();
  if (lock can&#39;t be acquired and the lock holding task is currently running) {
	while (lock_owner_running &amp;&amp; !need_resched()) {
		cpu_relax();
	}
  }
  enable_preempt();
  acquire_lock_or_sleep();
}
</code></pre><p>The lock path does exactly what I described. <code>cpu_relax()</code> is arch specific which is called when the CPU has to do nothing but wait. It gives hints to the CPU that it can put itself into an idle state or use its resources for someone else. For x86, it involves calling the <a href="https://en.wikipedia.org/wiki/HLT">halt instruction</a>.</p>
<p>What I noticed is the Ftrace latency tracer complained about a long delay in the preempt disabled path of mutex_lock for one of my tests, and I made some <a href="http://www.spinics.net/lists/linux-rt-users/msg15022.html">noise</a> about it on the mailing list. Disabling preemption for long periods is generally a bad thing to do because during this duration, no other task can be scheduled on the CPU. However, Steven <a href="http://www.spinics.net/lists/linux-rt-users/msg15025.html">pointed out that</a> for this particular case, since we&rsquo;re checking for need_resched() and breaking out of the loop, we should be Ok. What would happen is, the scheduling timer interrupt (which calls <code>scheduler_tick()</code> I mentioned earlier) comes in and checks if higher priority tasks need CPU, and if they do, it sets <code>TIF_NEED_RESCHED</code>. Once the timer interrupt returns to our tightly spinning loop in mutex_lock, we would break out of the loop having noticed <code>need_resched()</code> and, re-enable preemption as shown in the code above. Thus the long duration of preemption doesn&rsquo;t turn out to be a problem as long tasks that need CPU are prioritized correctly. <code>need_resched()</code> achieved this fairness.</p>
<p>Next time you see <code>if (need_resched())</code> in kernel code, you&rsquo;ll have a better idea why its there :). Let me know your comments if any.</p>
</div>

    </article>

    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">Tying 2 voltage sources/signals together</h1>
    
    
      <p class="meta">
        
<time datetime="2015-12-25T14:51:29-06:00" pubdate data-updated="true">Dec 25, 2015</time>

        
           | <a href="http://localhost:1313/blog/2015/12/25/tying-2-voltage-sources/signals-together/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/2015/12/25/tying-2-voltage-sources/signals-together/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>Recently I <a href="http://electronics.stackexchange.com/questions/207492/how-are-conflicts-between-voltage-sources-or-signals-resolved/207496">asked</a> a question on StackExchange about what happens when 2 voltage signals are tied together. What&rsquo;s the resultant voltage and what decides this voltage? The whole train of thought started when I was trying to contemplate what happens when you use pull-ups on signals that are not Open Drain.</p>
<p>I create and simulated a Circuit with the same scenario in LTSpice. &ldquo;V&rdquo; is the voltage between the &ldquo;+&rdquo; terminals of V1 and V2 and its shown on the right of the simulation. We will confirm the simulation result by doing some math later.{% img /images/voltage-conflict/voltage-conflict-1.png %}</p>
<p>The question is what is the voltage across the load after hooking them up together. And what do the currents look like? Is there a current flowing between the 2 sources as well (apart from the current flowing to the load) because 5v &gt; 1.8v?
The simulator refuses to do a simulation without your author adding an internal resistance to the voltage sources first. All voltages sources have certain internal resistances, so that&rsquo;s fair. This can be considered analogous to having a voltage signal with a certain resistance along its path which limits its current sourcing (or sinking) capabilities.</p>
<p>So I added 1k resistances internally, normally the resistance of a voltage source is far less than this. AA batteries have just 0.1-0.2ohms.
Now the circuit looks something like this: {% img /images/voltage-conflict/voltage-conflict-2.png %}</p>
<p>One can simply apply <a href="https://en.wikipedia.org/wiki/Kirchhoff%27s_circuit_laws#Kirchhoff.27s_current_law_.28KCL.29">Kirchoff&rsquo;s current law</a> to the above circuit, the direction of currents would be as in the circuit. I1 and I2 are the currents flowing through R2 and R1 respectively.</p>
<p>By Kirchoff&rsquo;s law, All the current entering the node labeled V will be equal to the current exiting it even if the currents are not in the actual direction shown above. From this we see:</p>
<pre tabindex="0"><code>I1 = (1.8 - V) / 1k
I2 = (5 - V)   / 1k
I3 = (V - 0)   / 10k

I3 = I2 + I1
V / 10k  = ((1.8 - V) / 1k) + ((5 - V) / 1k)
V = 3.2381v
</code></pre><p>Fom this we see the voltage at V is somewhere between 5 and 1.8v. Infact, where it is between 5 and 1.8 depends on how strong or weak the resistances associated with the sources are. If the resistances are lower, then the sources have more of an influence and vice versa. An interesting observation is I1 is negative if you plug V=3.2v in the above equation. This means the current for voltage source V2 (the 1.8v voltage source) is actually flowing into it rather than out of it (its being sinked) and so I1 is actually opposite in direction to the picture shown above.</p>
<p>A simpler case is having 2 voltage sources of the exact same voltage values, in this case the circuit would look like:{% img /images/voltage-conflict/voltage-conflict-3.png %}</p>
<p><a href="https://en.wikipedia.org/wiki/Th%C3%A9venin%27s_theorem">Thevenin&rsquo;s theorem</a>  provides an easy simplication into the following, where the equivalent voltage source value is the same but the series resistance is now halved. This results in the following circuit:
{% img /images/voltage-conflict/voltage-conflict-4.png %}</p>
<p>Now you can use the <a href="https://en.wikipedia.org/wiki/Voltage_divider">Voltage divider</a> concept and easily solve this:</p>
<pre tabindex="0"><code>V = V2 * (R2 / (R1 + R2) )
  = 1.8v * ( 10k / (10k + 0.5k) )
  = 1.7142v
</code></pre><p>As you would notice, the 1k resistance dropped around 0.085v of voltage before getting to the 10k load.
Thanks for reading. Please leave your comments or inputs below.</p>
</div>

    </article>

    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">MicroSD card remote switch</h1>
    
    
      <p class="meta">
        
<time datetime="2014-06-04T06:12:55-05:00" pubdate data-updated="true">Jun 4, 2014</time>

        
           | <a href="http://localhost:1313/blog/2014/06/04/microsd-card-remote-switch/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/2014/06/04/microsd-card-remote-switch/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>Recently, I&rsquo;ve been wanting to remotely be able to program a MicroSD card with a new bootloader or filesystem <em>without</em> removing the card from its embedded target board (such as a Beaglebone or Pandaboard). Due to the lack of any such existing tools, I decided to design my own board. Finally have got it working, below are some pictures and a screencast demo video of the switcher in action! I sprinkled some power and status LED to show the user what&rsquo;s going on.</p>
<p>The base board requires two <a href="https://www.sparkfun.com/products/9419">SparkFun MicroSD sniffers</a>. The card cage on the sniffer is unused for my purposes. The switcher is controlled through an <a href="https://www.sparkfun.com/products/9717">FTDI cable</a>.
I also <a href="https://github.com/joelagnel/microsd-switch/blob/master/sw/switch.c">wrote up</a> a <code>switch</code> program to control the switcher with libftdi. You just have to pass to it the FTDI&rsquo;s serial number and whether you want to switch to host-mode (for programming the card) or target-mode (for booting the programmed card).
<a href="https://github.com/joelagnel/microsd-switch">Hardware design files</a> are available under a CC-BY-NC-SA 3.0 license.</p>
<p>Screencast{% youtube StpIihVQ7oM %}</p>
<p>Pictures
{% img <a href="https://raw.githubusercontent.com/joelagnel/microsd-switch/master/board-pics/microsd-inaction/photo5.jpg">https://raw.githubusercontent.com/joelagnel/microsd-switch/master/board-pics/microsd-inaction/photo5.jpg</a> %}
{% img <a href="https://raw.githubusercontent.com/joelagnel/microsd-switch/master/board-pics/microsd-inaction/photo2.jpg">https://raw.githubusercontent.com/joelagnel/microsd-switch/master/board-pics/microsd-inaction/photo2.jpg</a> %}
{% img <a href="https://raw.githubusercontent.com/joelagnel/microsd-switch/master/board-pics/front.png">https://raw.githubusercontent.com/joelagnel/microsd-switch/master/board-pics/front.png</a> %}</p>
<p>Hope you enjoyed it, let me know what yout think in the comments:)</p>
</div>

    </article>

    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">Linux Spinlock Internals</h1>
    
    
      <p class="meta">
        
<time datetime="2014-05-07T22:42:45-05:00" pubdate data-updated="true">May 7, 2014</time>

        
           | <a href="http://localhost:1313/blog/2014/05/07/linux-spinlock-internals/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/2014/05/07/linux-spinlock-internals/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>This article tries to clarify how spinlocks are implemented in the Linux kernel and how they should be used correctly in the face of preemption and interrupts. The focus of this article will be more on basic concepts than details, as details tend to be forgotten more easily and shouldn&rsquo;t be too hard to look up although attention is paid to it to the extent that it helps understanding.</p>
<p>Fundamentally somewhere in <code>include/linux/spinlock.h</code>, a decision is made on which spinlock header to pull based on whether SMP is enabled or not:</p>
<pre tabindex="0"><code>#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
# include &lt;linux/spinlock_api_smp.h&gt;
#else
# include &lt;linux/spinlock_api_up.h&gt;
#endif
</code></pre><p>We&rsquo;ll go over how things are implemented in both the SMP (Symmetric Multi-Processor) and UP (Uni-Processor) cases.</p>
<p>For the SMP case, <code>__raw_spin_lock*</code> functions in <code>kernel/locking/spinlock.c</code> are called when one calls some version of a <code>spin_lock</code>.</p>
<p>Following is the definition of the most basic version defined with a macro:</p>
<pre tabindex="0"><code>#define BUILD_LOCK_OPS(op, locktype)                                    \
void __lockfunc __raw_##op##_lock(locktype##_t *lock)                   \
{                                                                       \
        for (;;) {                                                      \
                preempt_disable();                                      \
                if (likely(do_raw_##op##_trylock(lock)))                \
                        break;                                          \
                preempt_enable();                                       \
                                                                        \
                if (!(lock)-&gt;break_lock)                                \
                        (lock)-&gt;break_lock = 1;                         \
                while (!raw_##op##_can_lock(lock) &amp;&amp; (lock)-&gt;break_lock)\
                        arch_##op##_relax(&amp;lock-&gt;raw_lock);             \
        }                                                               \
        (lock)-&gt;break_lock = 0;                                         \
}                                                                       \
</code></pre><p>The function has several imporant bits. First it disables preemption on line 5, then tries to <em>atomically</em> acquire the spinlock on line 6. If it succeeds it breaks from the <code>for</code> loop on line 7, leaving preemption disabled for the duration of crticial section being protected by the lock. If it didn&rsquo;t succeed in acquiring the lock (maybe some other CPU grabbed the lock already), it enables preemption back and spins till it can acquire the lock keeping <em>preemption enabled during this period</em>. Each time it detects that the lock can&rsquo;t be acquired in the <code>while</code> loop, it calls an architecture specific relax function which has the effect executing some variant of a <code>no-operation</code> instruction that causes the CPU to execute such an instruction efficiently in a lower power state. We&rsquo;ll talk about the <code>break_lock</code> usage in a bit. Soon as it knows the lock is free, say the <code>raw_spin_can_lock(lock)</code> function returned 1, it goes back to the beginning of the <code>for</code> loop and tries to acquire the lock again.</p>
<p>What&rsquo;s important to note here is the reason for keeping preemption enabled (we&rsquo;ll see in a bit that for UP configurations, this is not done). While the kernel is spinning on a lock, other processes shouldn&rsquo;t be kept from preempting the spinning thread. The lock in these cases have been acquired on a <em>different CPU</em> because (assuming bug free code) it&rsquo;s impossible the current CPU which is trying to grab the lock has already acquired it, because preemption is disabled on acquiral. So it makes sense for the spinning kernel thread to be preempted giving others CPU time.
It is also possible that more than one process on the current CPU is trying to acquire the same lock and spinning on it, in this case the kernel gets continuously preempted between the 2 threads fighting for the lock, while some other CPU in the cluster happily holds the lock, hopefully for not too long.</p>
<p>That&rsquo;s where the <code>break_lock</code> element in the lock structure comes in. Its used to signal to the lock-holding processor in the cluster that there is someone else trying to acquire the lock. This can cause the lock to released early by the holder if required.</p>
<p>Now lets see what happens in the UP (Uni-Processor) case.</p>
<p>Believe it or not, it&rsquo;s really this simple:</p>
<pre tabindex="0"><code>#define ___LOCK(lock) \
  do { __acquire(lock); (void)(lock); } while (0)

#define __LOCK(lock) \
  do { preempt_disable(); ___LOCK(lock); } while (0)

// ....skipped some lines.....
#define _raw_spin_lock(lock)                    __LOCK(lock)
</code></pre><p>All that needs to be done is to disable preemption and acquire the lock. The code really doesn&rsquo;t do anything other than disable preemption. The references to the <code>lock</code> variable are just to suppress compiler warnings as mentioned in comments in the source file.</p>
<p>There&rsquo;s no spinning at all here like the UP case and the reason is simple: in the SMP case, remember we had agreed that while a lock is <em>acquired</em> by a particular CPU (in this case just the 1 CPU), no other process on that CPU should have acquired the lock. How could it have gotten a chance to do so with preemption disabled on that CPU to begin with?</p>
<p>Even if the code is buggy, (say the same process tries to acquires the lock twice), it&rsquo;s still impossible that 2 <em>different processes</em> try to acquire the same lock on a Uni-Processor system considering preemption is disabled on lock acquiral. Following that idea, in the Uni-processor case, since we are running on only 1 CPU, all that needs to be done is to disable preemption, since the fact that we are being allowed to disable preemption to begin with, means that no one else has acquired the lock. Works really well!</p>
<h2 id="sharing-spinlocks-between-interrupt-and-process-context">Sharing spinlocks between interrupt and process-context</h2>
<p>It is possible that a critical section needs to be protected by the same lock in both an interrupt and in non-interrupt (process) execution context in the kernel. In this case <code>spin_lock_irqsave</code> and the <code>spin_unlock_irqrestore</code> variants have to be used to protect the critical section. This has the effect of disabling interrupts on the  executing CPU. Imagine what would happen if you just used <code>spin_lock</code> in the process context?</p>
<p>Picture the following:</p>
<ol>
<li>Process context kernel code acquires <em>lock A</em> using <code>spin_lock</code>.</li>
<li>While the lock is held, an interrupt comes in on the same CPU and executes.</li>
<li>Interrupt Service Routing (ISR) tries to acquire <em>lock A</em>, and spins continuously waiting for it.</li>
<li>For the duration of the ISR, the Process context is blocked and never gets a chance to run and free the lock.</li>
<li>Hard lock up condition on the CPU!</li>
</ol>
<p>To prevent this, the process context code needs call <code>spin_lock_irqsave</code> which has the effect of disabling interrupts on that particular CPU along with the regular disabling of preemption we saw earlier <em>before</em> trying to grab the lock.</p>
<p>Note that the ISR can still just call <code>spin_lock</code> instead of <code>spin_lock_irqsave</code> because interrupts are disabled anyway during ISR execution. Often times people use <code>spin_lock_irqsave</code> in an ISR, that&rsquo;s not necessary.</p>
<p>Also note that during the executing of the critical section protected by <code>spin_lock_irqsave</code>, the interrupts are only disabled on the executing CPU. The same interrupt can come in on a different CPU and the ISR will be executed there, but that will not trigger the hard lock condition I talked about, because the process-context code is not blocked and can finish executing the locked critical section and release the lock while the ISR spins on the lock on a different CPU waiting for it. The process context does get a chance to finish and free the lock causing no hard lock up.</p>
<p>Following is what the <code>spin_lock_irqsave</code> code looks like for the SMP case, UP case is similar, look it up. BTW, the only difference here compared to the regular <code>spin_lock</code> I described in the beginning are the <code>local_irq_save</code> and <code>local_irq_restore</code> that accompany the <code>preempt_disable</code> and <code>preempt_enable</code> in the lock code:</p>
<pre tabindex="0"><code>#define BUILD_LOCK_OPS(op, locktype)                                    \
unsigned long __lockfunc __raw_##op##_lock_irqsave(locktype##_t *lock)  \
{                                                                       \
        unsigned long flags;                                            \
                                                                        \
        for (;;) {                                                      \
                preempt_disable();                                      \
                local_irq_save(flags);                                  \
                if (likely(do_raw_##op##_trylock(lock)))                \
                        break;                                          \
                local_irq_restore(flags);                               \
                preempt_enable();                                       \
                                                                        \
                if (!(lock)-&gt;break_lock)                                \
                        (lock)-&gt;break_lock = 1;                         \
                while (!raw_##op##_can_lock(lock) &amp;&amp; (lock)-&gt;break_lock)\
                        arch_##op##_relax(&amp;lock-&gt;raw_lock);             \
        }                                                               \
        (lock)-&gt;break_lock = 0;                                         \
        return flags;                                                   \
}                                                                       \
</code></pre><p>Update:
Further clean up of break_lock has happened <a href="https://lore.kernel.org/patchwork/patch/856271/">in this
patch</a>. If caller needs to
know if lock is contended, they can do so directly by using
arch_spin_is_contended. So there&rsquo;s no longer a need for the break_lock variable
in the lock structure.</p>
<p>Hope this post made a few things more clear, there&rsquo;s a lot more to spinlocking. A good reference is <a href="https://www.kernel.org/pub/linux/kernel/people/rusty/kernel-locking/">Rusty&rsquo;s Unreliable Guide To Locking</a>.</p>
</div>

    </article>

    <article>
Most Recent Post:
      
  <header>
    
      <h1 class="entry-title">Studying cache-line sharing effects on SMP systems</h1>
    
    
      <p class="meta">
        
<time datetime="2014-04-24T20:28:24-05:00" pubdate data-updated="true">Apr 24, 2014</time>

        
           | <a href="http://localhost:1313/blog/2014/04/24/studying-cache-line-sharing-effects-on-smp-systems/#disqus_thread"
             data-disqus-identifier="http://localhost:1313/blog/2014/04/24/studying-cache-line-sharing-effects-on-smp-systems/">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>Having read the chapter on counting and per-CPU counters in <a href="http://www.lulu.com/shop/paul-e-mckenney/is-parallel-programming-hard-and-if-so-what-can-you-do-about-it-first-bw-print-edition/paperback/product-21562459.html">Paul Mckenney&rsquo;s recent book</a>, I thought I would do a small experiment to check how good or bad it would be if those per-CPU counters were close to each other in memory.</p>
<p>Paul talks about using one global shared counter for N threads on N CPUs, and the effects it can have on the cache. Each CPU core&rsquo;s cache in an SMP system will need exclusive rights on a specific cache line of memory, before it can do the write. This means that, at any given time <em>only one</em> CPU can and should do a write to that part of memory.</p>
<p>This is accomplished typically by an invalidate protocol, where each CPU needs to do some inter-processor communication before it can assume it has exclusive access to that cache line, and also read any copies that may still be in some other core&rsquo;s cache and not in main memory. This is an expensive operation that is to be avoided at all costs!</p>
<p>Then Paul goes about saying, OK- let&rsquo;s have a per-thread counter, and have each core increment it independently, and when we need a read out, we would grab a lock and add all of the individual counters together. This works great, assuming each per-thread counter is separated by atleast a cache line. That&rsquo;s guaranteed, when one uses the <code>__thread</code> primitive nicely separating out the memory to reduce cache line sharing effects.</p>
<p>So I decided to flip this around, and have per-thread counters that were closely spaced and do some counting with them. Instead of using <code>__thread</code>, I created an array of counters, each element belonging to some thread. The counters are still separate and not shared, but they may still be in shared cache-line causing the nasty effects we talked about, which I wanted to measure.</p>
<p><a href="https://github.com/joelagnel/smp-experiments/blob/05afb2db4fea1c6c0b4614c180186c10627a341a/cache-sharing.c">My program</a> sets up N counting threads, and assumes its running each of them on a single core on typical multicore system.  Various iterations of per-thread counting is done, with the counters separated by increasing powers of 2 each iteration. After each iteration, I stop all threads, add the per-thread counter values and report the result.</p>
<p>Below are the results of running the program on 3 different SMP systems (2 threads on 2 CPUs, sorry I don&rsquo;t have better multi-core hardware ATM):</p>
<p>Effect of running on a reference ARM dual-core Cortex-A9 system:
{% img /images/cache-sharing/a9-counts.jpeg %}</p>
<p>Notice the jump in through-put once the separation changes from 16 to 32 bytes. That gives us a good idea that the L1 cache line size on Cortex-A9 systems is 32 bytes (8 words). Something the author didn&rsquo;t know for sure in advance (I initially thought it was 64-bytes).</p>
<p>Effect of running on a reference ARM dual-core Cortex-A15 system:
{% img /images/cache-sharing/a15-counts.jpeg %}</p>
<p>L1 Cache-line size of Cortex A-15 is 64 bytes (8 words). Expected jump for a separation of 64 bytes.</p>
<p>Effect of running on a x86-64 i7-3687U dual-core CPU:
{% img /images/cache-sharing/x86-counts.jpeg %}.</p>
<p>L1 Cache-line size of this CPU is 64 bytes too (8 words). Expected jump for a separation of 64 bytes.</p>
<p>This shows your parallel programs need to take care of cache-line alignment to avoid false-sharing effects. Also, doing something like this in your program is an indirect way to find out what the cache-line size is for your CPU, or a direct way to get fired, whichever way you want to look at it. ;)</p>
</div>

    </article>



<div class="pagination">
  

  
    <a href="/page/2/">Next Post&raquo;</a>
  
</div>



</div>


<aside class="sidebar">
  <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="http://localhost:1313/blog/2023/06/25/svm-and-vectors-for-the-curious/">SVM and vectors for the curious</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2018/12/22/dumping-user-and-kernel-stacks-on-kernel-events/">Dumping User and Kernel stacks on Kernel events</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2018/02/10/usdt-for-reliable-userspace-event-tracing/">USDT for reliable Userspace event tracing</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2016/12/31/armv8-flamegraph-and-nmi-support/">ARMv8: flamegraph and NMI support</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2016/06/18/ftrace-events-mechanism/">Ftrace events mechanism</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2016/03/20/tif_need_resched-why-is-it-needed/">TIF_NEED_RESCHED: why is it needed</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2015/12/25/tying-2-voltage-sources/signals-together/">Tying 2 voltage sources/signals together</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2014/06/04/microsd-card-remote-switch/">MicroSD card remote switch</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2014/05/07/linux-spinlock-internals/">Linux Spinlock Internals</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2014/04/24/studying-cache-line-sharing-effects-on-smp-systems/">Studying cache-line sharing effects on SMP systems</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/2014/04/22/design-of-fork-followed-by-exec-in-linux/">Design of fork followed by exec in Linux</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/"></a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/bpfd-running-bcc-tools-remotely-across-systems/">BPFd- Running BCC tools remotely across systems</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/c-rvalue-references/">C&#43;&#43; rvalue references</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/categories/">Categories</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/joel/">false</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/figuring-out-herd7-memory-models/">Figuring out herd7 memory models</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/getting-youcompleteme-working-for-kernel-development/">Getting YouCompleteMe working for kernel development</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/gus-global-unbounded-sequences/">GUS (Global Unbounded Sequences)</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/archives/">List of articles</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/making-sense-of-scheduler-deadlocks-in-rcu/">Making sense of scheduler deadlocks in RCU</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/modeling-lack-of-store-ordering-using-pluscal-and-a-wishlist/">Modeling (lack of) store ordering using PlusCal - and a wishlist</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/on-workings-of-hrtimers-slack-time-functionality/">On workings of hrtimer&#39;s slack time functionality</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/powerpc-stack-guard-false-positives-in-linux-kernel/">PowerPC stack guard false positives in Linux kernel</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/rcu-and-dynticks-idle-mode/">RCU and dynticks-idle mode</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/rcu-preempt-what-happens-on-a-context-switch/">RCU-preempt: What happens on a context switch</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/resources/">Resources</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/selinux-debugging-on-chromeos/">SELinux Debugging on ChromeOS</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/single-stepping-the-kernels-c-code/">Single-stepping the kernel&#39;s C code</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/srcu-state-double-scan/">SRCU state double scan</a>
      </li>
    
      <li class="post">
        <a href="http://localhost:1313/blog/1/01/01/understanding-hazard-pointers/">Understanding Hazard Pointers</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>About Me</h1>
  <p>A little something about me.</p>
</section>
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2025 - Joel Fernandes -
  <span class="credit">Powered by <a href="https://gohugo.io">Hugo</a></span>
</p></footer>
  
</body>
</html>